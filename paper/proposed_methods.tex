% ============================================================================
%  Proposed Methods â€” Clustering, Refinement, Registration, Features
% ============================================================================

\section{Proposed Methods}
\label{sec:proposed-methods}

We propose four categories of improvements over the baseline: alternative clustering strategies (Sect.~\ref{sec:proposed-clustering}), principled refinement (Sect.~\ref{sec:proposed-refinement}), registration methods (Sect.~\ref{sec:proposed-registration}), and deformation-invariant features (Sect.~\ref{sec:proposed-features}).


% ============================================================================
\subsection{Alternative Clustering Methods}
\label{sec:proposed-clustering}

The baseline binarises NLFA values into edges, discarding the graded similarity signal.
We propose three alternatives operating on the continuous NLFA matrix.


\paragraph{HDBSCAN with NFA-derived distance.}
\label{sec:hdbscan}
HDBSCAN~\cite{mcinneshdbscan2017} provides density-based clustering that preserves the continuous NLFA signal, automatically determines $K$, and identifies noise points.
We define the distance $d(I_i, I_j) = D_{\max} - \frac{1}{2}[\text{NLFA}(I_i, I_j) + \text{NLFA}(I_j, I_i)]$ with sentinel $D_{\max} = 50$.
The mutual reachability distance
\begin{equation}
    d_{\text{mr}}(I_i, I_j) = \max\!\bigl(d_{\text{core}}(I_i),\; d_{\text{core}}(I_j),\; d(I_i, I_j)\bigr)
    \label{eq:mutual-reachability}
\end{equation}
inflates distances in sparse regions, making the algorithm robust to variable cluster sizes.
Flat clusters are extracted via Excess of Mass (EOM) selection on the condensed tree, using cluster stability $\text{Stab}(C) = \sum_{I_i \in C} (\lambda_{\text{death}}(I_i, C) - \lambda_{\text{birth}}(C))$ where $\lambda = 1/d_{\text{mr}}$.


\paragraph{Affinity Propagation.}
\label{sec:affinity-propagation}
AP~\cite{frey2007clustering} operates directly on the symmetrised NLFA matrix, simultaneously identifying exemplars (prototypical instances) and assignments without requiring~$K$.
Responsibility $r(i,k) \leftarrow s(i,k) - \max_{k' \neq k}\{a(i,k') + s(i,k')\}$ and availability messages are passed iteratively with damping $\lambda \in [0.5, 0.9]$ until convergence.
The self-similarity (preference) $p = \text{median}(s)$ controls the number of clusters.


\paragraph{MRF with Loopy Belief Propagation.}
\label{sec:mrf}
A Markov Random Field~\cite{yedidia2003understanding} jointly optimises cluster assignments using both visual and linguistic signals.
The energy $E(\mathbf{z}) = \sum_i \psi_i(z_i) + \sum_{(i,j)} \psi_{ij}(z_i, z_j)$ combines OCR unary potentials $\psi_i(z_i{=}c) = -\log P_{\text{OCR}}(I_i{=}c)$ with generalised Potts pairwise potentials:
\begin{equation}
    \psi_{ij}(z_i, z_j) = \begin{cases} -\beta \cdot \text{NLFA}(I_i, I_j) & \text{if } z_i = z_j, \\ 0 & \text{otherwise,} \end{cases}
    \label{eq:potts}
\end{equation}
where $\beta = 1/\text{median}(\text{NLFA})$.
This subsumes the three baseline refinement stages: unary potentials encode the OCR signal (Stage~2), pairwise potentials capture visual similarity (Stage~1), and global energy minimisation handles singletons (Stage~3).


% ============================================================================
\subsection{Principled Refinement: K-Medoids with Split/Merge}
\label{sec:proposed-refinement}

The three ad hoc refinement stages are replaced by a single iterative algorithm.
Starting from the baseline partition, medoids are computed as $m_C = \argmin_{I_i \in C} \sum_{I_j \in C} d(I_i, I_j)$.

\emph{Reassignment:} Each character is compared to the $k$-nearest alternative medoids; if a better match exists, it is reassigned.
\emph{Split:} Clusters with high average intra-cluster distance ($\bar{d}_C > \mu_0 + 2\sigma_0$) are bisected via the Fiedler vector of the normalised graph Laplacian.
\emph{Merge:} Cluster pairs whose medoids are NFA-similar and share $>30\%$ cross-cluster edges are merged.
Iteration (Reassign $\to$ Split $\to$ Merge $\to$ Recompute medoids) converges in 3--5 rounds.


% ============================================================================
\subsection{Registration Methods}
\label{sec:proposed-registration}

Given a cluster $C = \{I_1, \ldots, I_m\}$, we require a common coordinate frame for template computation.

\paragraph{MST-based hierarchical alignment.}
\label{sec:mst-alignment}
A $k$-NN alignment graph ($k{=}5$) is built from pairwise registration residuals $w(I_i, I_j) = \min_T \mathcal{L}_\text{Lorentz}(I_i \circ T, I_j)$.
The minimum spanning tree, rooted at the cluster medoid, propagates transformations root-to-leaves: $T_\text{child}^\text{global} = T_\text{parent}^\text{global} \circ T_{\text{child} \to \text{parent}}$.
The template is the pixel-wise median of aligned images.
Error at depth $L$ is bounded by $L \cdot \delta + O(L^2 \delta^2)$; 1--2 refinement rounds against the current template mitigate drift.

\paragraph{Congealing.}
\label{sec:congealing}
Joint optimisation of all transformations and a latent template $\bar{I}$~\cite{learned2004data}:
\begin{equation}
    \min_{\bar{I},\, \{T_i\}} \sum_{i=1}^{m} \mathcal{L}\!\bigl(I_i \circ T_i,\; \bar{I}\bigr) + \lambda \sum_{i=1}^{m} \|T_i - \text{Id}\|_F^2.
    \label{eq:congealing}
\end{equation}
Alternating: fix $\{T_i\}$, update $\bar{I}$ via pixel-wise median (robust to outliers); fix $\bar{I}$, update each $T_i$ via IC registration.
Congealing produces sharper templates than the Fr\'echet mean, which blurs from residual misalignment.


% ============================================================================
\subsection{Deformation-Invariant Features}
\label{sec:proposed-features}

\paragraph{Shape Context descriptors.}
\label{sec:shape-context}
For each character, $N_s = 100$ contour points are sampled uniformly~\cite{belongie2002shape}.
At each point, the distribution of relative vectors in log-polar coordinates is computed (12~angular $\times$ 5~radial bins).
Character distance is the minimum-cost Hungarian matching of per-point histograms using chi-squared distance.

\paragraph{Persistent homology.}
\label{sec:persistent-homology}
The sublevel set filtration of the distance transform tracks topological features ($H_0$: components, $H_1$: loops) across scales~\cite{edelsbrunner2008persistent}.
Persistent homology serves as a fast rejection step: topologically incompatible pairs (different number of significant loops) are rejected before the expensive NFA comparison, computed in $O(HW)$ per character via a single distance transform.
