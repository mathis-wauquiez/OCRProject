% ============================================================================
%  Experiments — Results and Evaluation
% ============================================================================

\section{Experiments}
\label{sec:experiments}


% ============================================================================
\subsection{Dataset}
\label{sec:dataset}

We evaluate on the \emph{Ying huan zhi lue} (Brief Records of the World, 1848), a Chinese book printed from individually carved woodblocks.
The corpus contains \nPages{} page images from which the extraction pipeline produces approximately \nPatches{} character patches.
Each character was uniquely engraved---no two occurrences share the same printing block---making this a maximally challenging scenario for visual clustering.

Ground-truth labels are obtained at two levels:
\begin{itemize}[nosep]
    \item \textbf{OCR predictions} ($\ell_\text{chat}$): Kraken model output, used during hyperparameter selection to avoid methodological leak.
    \item \textbf{Consensus labels} ($\ell_\text{consensus}$): agreement between OCR predictions and aligned transcription labels, used only for final evaluation.
\end{itemize}


% ============================================================================
\subsection{Evaluation Metrics}
\label{sec:eval-protocol}

Clustering quality is measured using the metrics summarised in Table~\ref{tab:metrics}.
Characters with unknown ground-truth labels are excluded from all metric computations.

\begin{table}[t]
    \centering
    \small
    \caption{Evaluation metrics and their interpretation.}
    \label{tab:metrics}
    \begin{tabular}{@{}lp{7.5cm}@{}}
        \toprule
        \textbf{Metric} & \textbf{Description} \\
        \midrule
        ARI~\cite{hubert1985comparing} & Chance-adjusted pairwise agreement; primary metric. \\
        NMI & Information-theoretic overlap; robust to different cluster counts. \\
        V-measure~\cite{rosenberg2007v} & Harmonic mean of homogeneity and completeness. \\
        Purity & Fraction of majority-label members per cluster (weighted by cluster size). \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Hyperparameter selection.}
The pipeline performs a grid sweep over epsilon thresholds ($\varepsilon \in \{1.5{\times}10^{-4}, 2{\times}10^{-4}, 2.5{\times}10^{-4}, 3.8{\times}10^{-4}, 10^{-3}\}$) and Leiden resolution ($\gamma \in \{1.0, 1.25, 1.5, 1.75, 2.0\}$) using the CPM partition type.
The configuration maximising ARI on OCR-derived labels is selected; final metrics are reported on consensus labels.


% ============================================================================
\subsection{Results}
\label{sec:results}

\begin{table}[t]
    \centering
    \small
    \caption{Clustering results on the \emph{Ying huan zhi lue} corpus.
    Metrics are computed on consensus labels after each refinement stage.
    $K$ denotes the number of clusters.}
    \label{tab:main-results}
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        \textbf{Stage} & \textbf{$K$} & \textbf{ARI} & \textbf{NMI} & \textbf{V-measure} & \textbf{Purity} \\
        \midrule
        Leiden (baseline)    & \nClustersBaseline  & \ARIBaseline  & \NMIBaseline  & \VmeasureBaseline  & \PurityBaseline  \\
        + Refinement (final) & \nClustersFinal     & \ARIFinal     & \NMIFinal     & \VmeasureFinal     & \PurityFinal     \\
        \midrule
        OCR-only baseline    & ---                  & \ARIocr       & \NMIocr       & \Vmeasureocr       & \Purityocr       \\
        \bottomrule
    \end{tabular}
\end{table}

Table~\ref{tab:main-results} reports clustering quality at each stage of the pipeline.
The Leiden partition provides the initial grouping; the two-stage refinement (Hausdorff splitting followed by label splitting) improves both purity and overall ARI.
The OCR-only baseline (clustering by predicted label) provides an informative comparison: it uses no visual similarity and relies entirely on the linguistic signal.

\paragraph{Graph topology.}
The NFA similarity graph contains \nNodes{} nodes and \nEdges{} edges (density~\graphDensity), with an average degree of~\avgDegree.
Isolated characters ($d{=}0$) are predominantly rare or damaged glyphs, while high-degree nodes correspond to frequent characters with many visually similar instances.


% ============================================================================
\subsection{Hyperparameter Sensitivity}
\label{sec:sensitivity}

% ── Epsilon sweep ──
\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{\figEpsilonSensitivity}
    \caption{Sensitivity to the NFA threshold $\varepsilon$.
    ARI is plotted as a function of $\varepsilon$ for each partition type.
    Error bars indicate standard deviation over the resolution sweep.}
    \label{fig:epsilon-sweep}
\end{figure}

\paragraph{NFA threshold.}
The epsilon parameter controls graph sparsity: smaller $\varepsilon$ yields stricter matching (fewer edges, more clusters), while larger $\varepsilon$ produces denser graphs.
Figure~\ref{fig:epsilon-sweep} shows ARI as a function of $\varepsilon$.
% TODO: Describe the optimal region and robustness.

% ── Split threshold sweep ──
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{\figSplitThreshold}
    \caption{Sensitivity to the Hausdorff split threshold $\tau_\text{split}$.
    \emph{Left:} clustering metrics vs.\ $\tau_\text{split}$; the best ARI is marked by a dashed line.
    \emph{Right:} total number of clusters and number of clusters actually split.}
    \label{fig:split-threshold}
\end{figure}

\paragraph{Split threshold.}
The Hausdorff dendrogram cut $\tau_\text{split}$ trades off cluster purity against fragmentation.
Figure~\ref{fig:split-threshold} shows that ARI peaks at an intermediate value of $\tau_\text{split}$: too low splits every cluster into singletons, while too high leaves impure clusters intact.
% TODO: Report the optimal value and comment on robustness.


% ============================================================================
\subsection{Ablation Studies}
\label{sec:ablations}

% ── A1: HOG configuration ──
\paragraph{HOG configuration (A1).}
We compare HOG descriptor configurations varying cell size, gradient smoothing $\sigma$, and number of orientation bins.
% Uncomment once generated:
% \input{figures/generated/ablation_a1_hog_config}

% ── A2: Partition type ──
\paragraph{Partition type (A2).}
We compare the CPM objective against modularity (RBConfiguration) under the Leiden algorithm.
% Uncomment once generated:
% \input{figures/generated/ablation_a2_partition_type}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{\figAblationPartition}
    \caption{CPM vs.\ modularity (RBConfiguration): ARI as a function of the resolution parameter $\gamma$.
    CPM consistently achieves higher ARI across the resolution range.}
    \label{fig:ablation-partition}
\end{figure}

% ── A4: Reciprocal edges ──
\paragraph{Reciprocal edges (A4).}
We evaluate the effect of the reciprocal condition (Eq.~\ref{eq:reciprocal}) on graph construction.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{\figAblationReciprocal}
    \caption{Reciprocal vs.\ non-reciprocal edge construction: ARI as a function of $\varepsilon$.
    The reciprocal condition improves clustering by removing asymmetric, unreliable edges.}
    \label{fig:ablation-reciprocal}
\end{figure}


% ============================================================================
\subsection{Qualitative Results}
\label{sec:qualitative}

% ── Cluster gallery ──
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{\figClusterGallery}
    \caption{Example clusters from the final partition.
    Each row shows members of one cluster, with the representative (highest degree centrality) highlighted with a green border.
    \emph{Top rows:} pure clusters with consistent character identity.
    \emph{Bottom rows:} impure clusters where visually similar but distinct characters were grouped together.}
    \label{fig:cluster-gallery}
\end{figure*}

Figure~\ref{fig:cluster-gallery} shows example clusters.
Pure clusters demonstrate that the pipeline reliably groups visually identical characters.
Impure clusters reveal failure modes: typically characters with very similar stroke structures that differ in a single stroke.

% ── Character catalogue (selected characters) ──
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{\figCharCatalogue}
    \caption{Character catalogue for selected characters.
    For each character, all patch occurrences are shown grouped by cluster
    (clusters sorted by size, largest first).  The representative patch
    (highest degree centrality) is highlighted with a coloured border.
    Green border~=~pure cluster ($\ge 0.95$~purity),
    orange border~=~impure cluster.
    % TODO: List the selected characters here.
    }
    \label{fig:char-catalogue}
\end{figure*}

Figure~\ref{fig:char-catalogue} presents a detailed character catalogue for selected characters, showing every occurrence grouped by its assigned cluster.
This view complements the cluster gallery by inverting the perspective: instead of clusters containing mixed characters, it shows how patches of a single character are distributed across clusters.

% ── Glossary (most frequent) ──
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{\figGlossary}
    \caption{Character glossary --- most frequent characters, sorted by occurrence count.
    Each cell displays the vectorised representative of one cluster along with the predicted label and count.}
    \label{fig:glossary}
\end{figure*}

The final glossary (Fig.~\ref{fig:glossary}) provides a comprehensive inventory of the most frequent character forms found in the book.
The frequency distribution follows a Zipfian law: a small number of characters account for the majority of occurrences.

% ── Glossary (least frequent) ──
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{\figGlossaryLeast}
    \caption{Character glossary --- least frequent characters, sorted by ascending occurrence count.
    These rare characters are the most challenging for clustering: with few examples, the pipeline has limited evidence for grouping.}
    \label{fig:glossary-least-frequent}
\end{figure*}

Figure~\ref{fig:glossary-least-frequent} displays the least frequent characters.
These are particularly challenging because the clustering pipeline has very few instances to form reliable groups, and many correspond to rare or specialised vocabulary.

% ── Glossary (intermediate cluster size) ──
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{\figGlossaryIntermediate}
    \caption{Character glossary --- characters whose main cluster has exactly two members.
    These intermediate-size clusters reveal borderline cases where the pipeline found a small but non-trivial grouping.}
    \label{fig:glossary-intermediate}
\end{figure*}

Figure~\ref{fig:glossary-intermediate} shows characters whose main cluster contains exactly two members.
These intermediate clusters are informative: they indicate cases where the pipeline found sufficient visual similarity to group two instances, but not enough evidence to attract additional members.

% ── Discrepancy statistics ──
\paragraph{Discrepancy analysis.}
% Uncomment once generated:
% \input{figures/generated/discrepancy_table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{\figDiscrepancy}
    \caption{Discrepancy between OCR predictions and ground-truth labels.
    \emph{Left:} distribution of patches across match categories.
    \emph{Right:} the most frequently confused character pairs (OCR prediction $\to$ ground truth).}
    \label{fig:discrepancy}
\end{figure}

Figure~\ref{fig:discrepancy} analyses the discrepancy between OCR predictions ($\ell_\text{chat}$) and ground-truth labels ($\ell_\text{consensus}$).
The confused-pairs chart reveals systematic OCR errors, typically between visually similar characters that differ in a single stroke.

% ── t-SNE cluster visualizations ──
\paragraph{Cluster structure (t-SNE).}
Selected clusters are visualised using t-SNE projections of their HOG descriptors (Figures~\ref{fig:tsne-a}--\ref{fig:tsne-d}).
These plots reveal the internal structure of clusters: pure clusters form tight, unimodal groups, while impure clusters often show distinct sub-populations that could be separated with finer resolution.

% TODO: Replace \figTsneA..D paths in main.tex with actual cluster filenames.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{\figTsneA}
    \caption{t-SNE projection of cluster A.
    % TODO: Fill in cluster ID, character label, purity, and size.
    }
    \label{fig:tsne-a}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{\figTsneB}
    \caption{t-SNE projection of cluster B.
    % TODO: Fill in cluster ID, character label, purity, and size.
    }
    \label{fig:tsne-b}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{\figTsneC}
    \caption{t-SNE projection of cluster C.
    % TODO: Fill in cluster ID, character label, purity, and size.
    }
    \label{fig:tsne-c}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{\figTsneD}
    \caption{t-SNE projection of cluster D.
    % TODO: Fill in cluster ID, character label, purity, and size.
    }
    \label{fig:tsne-d}
\end{figure}

% ── Alignment visualization ──
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{\figAlignmentViz}
    \caption{Transcription alignment visualisation for a sample page.
    Each patch is annotated with its OCR prediction and aligned transcription label.
    Colour coding: \textcolor{green!70!black}{green} = match,
    \textcolor{orange}{orange} = OCR mismatch (corrected by transcription),
    \textcolor{red}{red} = deleted (no transcription counterpart),
    \textcolor{gray}{grey} = unknown OCR prediction.}
    \label{fig:alignment-viz}
\end{figure*}

Figure~\ref{fig:alignment-viz} illustrates the Levenshtein-based alignment between OCR predictions and the external transcription for a representative page.
The colour coding highlights where the OCR model succeeded or failed, providing insight into the quality of the labelling process.

% ── Reverse manuscript ──
\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/generated/reverse_manuscript/reverse_with_bg_page_000.png}
        \caption{Vectorised characters overlaid on the original scan (faded).}
        \label{fig:reverse-with-bg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/generated/reverse_manuscript/reverse_no_bg_page_000.png}
        \caption{Vectorised characters on a clean background.}
        \label{fig:reverse-no-bg}
    \end{subfigure}
    \caption{Reverse typography output for a representative page.
    \textbf{(a)}~Vectorised character outlines (blue) are overlaid on the faded original scan, demonstrating alignment quality.
    \textbf{(b)}~The same vectorised characters rendered on a white background, illustrating the clean reconstruction achievable by the pipeline.}
    \label{fig:reverse-manuscript}
\end{figure*}

Figure~\ref{fig:reverse-manuscript} demonstrates the end-to-end reverse typography result: vectorised character outlines placed at their original positions reconstruct the full page layout.
The overlay view~(a) confirms that the extracted and vectorised characters align well with the original print, while the clean-background view~(b) showcases the vectorisation quality.
