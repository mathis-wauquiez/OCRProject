% ============================================================================
%  Experiments — Results and Evaluation
% ============================================================================

\section{Experiments}
\label{sec:experiments}

This section describes the dataset, evaluation protocol, and experimental results.


% ============================================================================
\subsection{Dataset}
\label{sec:dataset}

We evaluate on the \emph{Ying huan zhi lue} (Brief Records of the World, 1848), a Chinese book printed from individually carved woodblocks.
The corpus contains 40 page images from which the extraction pipeline produces approximately 13\,500 character patches.
Each character was uniquely engraved---no two occurrences share the same printing block---making this a maximally challenging scenario for visual clustering.

Ground-truth labels are obtained at two levels:
\begin{itemize}[nosep]
    \item \textbf{OCR predictions} ($\ell_\text{chat}$): Kraken model output, used during hyperparameter selection to avoid methodological leak.
    \item \textbf{Consensus labels} ($\ell_\text{consensus}$): agreement between OCR predictions and aligned transcription labels, used only for final evaluation.
\end{itemize}


% ============================================================================
\subsection{Evaluation Metrics}
\label{sec:eval-protocol}

Clustering quality is measured using the metrics summarised in Table~\ref{tab:metrics}.
Characters with unknown ground-truth labels are excluded from all metric computations.

\begin{table}[t]
    \centering
    \small
    \caption{Evaluation metrics and their interpretation.}
    \label{tab:metrics}
    \begin{tabular}{@{}lp{7.5cm}@{}}
        \toprule
        \textbf{Metric} & \textbf{Description} \\
        \midrule
        ARI~\cite{hubert1985comparing} & Chance-adjusted pairwise agreement; primary metric. \\
        NMI & Information-theoretic overlap; robust to different cluster counts. \\
        V-measure~\cite{rosenberg2007v} & Harmonic mean of homogeneity and completeness. \\
        Purity & Fraction of majority-label members per cluster (weighted by cluster size). \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Hyperparameter selection.}
The pipeline performs a grid sweep over epsilon thresholds ($\varepsilon \in \{2.5{\times}10^{-4}, 3.8{\times}10^{-4}, 5{\times}10^{-4}\}$), Leiden resolution ($\gamma = 1.0$), and partition types (RBConfiguration, CPM).
The configuration maximising ARI on OCR-derived labels is selected; final metrics are reported on consensus labels.


% ============================================================================
\subsection{Results}
\label{sec:results}

% ── Placeholder for main results table ──
% TODO: Fill with actual metric values from a pipeline run
\begin{table}[t]
    \centering
    \small
    \caption{Clustering results on the \emph{Ying huan zhi lue} corpus.
    Metrics are computed on consensus labels after each refinement stage.}
    \label{tab:main-results}
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        \textbf{Stage} & \textbf{$K$} & \textbf{ARI} & \textbf{NMI} & \textbf{V-measure} & \textbf{Purity} \\
        \midrule
        Leiden (baseline)    & ---  & ---  & ---  & ---  & ---  \\
        + Hausdorff split    & ---  & ---  & ---  & ---  & ---  \\
        + Label split        & ---  & ---  & ---  & ---  & ---  \\
        \bottomrule
    \end{tabular}
\end{table}

Table~\ref{tab:main-results} reports clustering quality at each stage of the pipeline.
The Leiden partition provides the initial grouping; the Hausdorff split breaks impure clusters by visual dissimilarity, typically increasing the number of clusters~$K$ while improving purity; the label split further enforces label consistency within clusters.

% ── Placeholder for graph topology ──
\paragraph{Graph topology.}
% TODO: Fill from clustering sweep report
The NFA similarity graph contains $N$ nodes and $|E|$ edges (density~$\rho$), with an average degree of~$\bar{d}$ and $C$ connected components.
The degree distribution characterises the connectivity structure: isolated characters ($d{=}0$) are predominantly rare or damaged glyphs, while high-degree nodes correspond to frequent characters with many visually similar instances.


% ============================================================================
\subsection{Hyperparameter Sensitivity}
\label{sec:sensitivity}

% ── Placeholder for epsilon sweep figure ──
% TODO: Generate figure from sweep results
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/epsilon_sweep.pdf}
%     \caption{ARI as a function of the NFA threshold $\varepsilon$ for both partition types.
%     Dashed lines indicate OCR-label ARI (used for selection); solid lines show consensus-label ARI (final evaluation).}
%     \label{fig:epsilon-sweep}
% \end{figure}

\paragraph{NFA threshold.}
The epsilon parameter controls graph sparsity: smaller $\varepsilon$ yields stricter matching (fewer edges, more clusters), while larger $\varepsilon$ produces denser graphs.
% TODO: Describe the sensitivity curve and optimal epsilon.

\paragraph{Split threshold.}
The Hausdorff dendrogram cut $\tau_\text{split}$ trades off cluster purity against fragmentation.
% TODO: Describe the ARI curve across split thresholds.


% ============================================================================
\subsection{Qualitative Results}
\label{sec:qualitative}

% ── Placeholder for qualitative figure ──
% TODO: Generate figure showing example clusters
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/cluster_gallery.pdf}
%     \caption{Example clusters from the glossary.  Each row shows all members of one cluster, with the representative (highest degree centrality) highlighted.
%     Left: pure clusters with consistent character identity.
%     Right: impure clusters where visually similar but distinct characters are grouped together.}
%     \label{fig:cluster-gallery}
% \end{figure}

The character glossary provides a visual summary of the clustering output.
% TODO: Describe representative examples: pure clusters, split clusters, remaining errors.


% ============================================================================
\subsection{Comparison with OCR-Only Clustering}
\label{sec:ocr-baseline}

% ── Placeholder for comparison table ──
% TODO: Fill with OCR-only baseline metrics
\begin{table}[t]
    \centering
    \small
    \caption{Comparison with the OCR-only baseline (clustering by predicted label).}
    \label{tab:ocr-comparison}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Method} & \textbf{ARI} & \textbf{NMI} & \textbf{V-measure} & \textbf{Purity} \\
        \midrule
        OCR-only (Kraken)     & --- & --- & --- & --- \\
        Ours (full pipeline)  & --- & --- & --- & --- \\
        \bottomrule
    \end{tabular}
\end{table}

Clustering by OCR-predicted label provides an informative baseline: it uses no visual similarity and relies entirely on the linguistic signal.
% TODO: Discuss the comparison.
