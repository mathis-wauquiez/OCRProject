% ============================================================================
%  Methodology Section â€” Historical Document Character Clustering Pipeline
% ============================================================================

\section{Methodology}
\label{sec:methodology}

Our pipeline processes scanned historical document images to produce a vectorised character glossary through four stages: (1)~character extraction, (2)~preprocessing including vectorisation, OCR, and feature computation, (3)~\textit{a contrario} matching and graph-based clustering, and (4)~post-clustering refinement and glossary construction.
Figure~\ref{fig:pipeline-overview} presents an overview.

\input{figures/fig_pipeline_overview}


% ============================================================================
\subsection{Character Extraction}
\label{sec:extraction}

We use CRAFT~\cite{baek2019character} (\textit{Character Region Awareness for Text Detection}), a fully convolutional network producing per-pixel text score maps $S_\text{text}(x, y) \in [0,1]$.
We use pre-trained weights without fine-tuning, with magnification ratio~$5.0$ (canvas size 1280\,px).

Connected components are extracted via dual-threshold watershed: pixels with $S_\text{text} \ge \tau_\text{text} = 0.6$ serve as seeds, and basins expand to a lower mask threshold $\tau_\text{mask} = 0.3$.
Nearby components (centroids $< 8$\,px apart) are merged to reconnect split radicals of composite characters.

In parallel, the page is binarised (Otsu's method) and binary connected components are associated with CRAFT detections via negative Mahalanobis distance using the CRAFT region's inertia tensor.
Filters discard components outside the expected size range ($30 \times 30$ to $150 \times 150$\,px), with extreme aspect ratios ($>3$), or insufficient area ($<200$\,px$^2$).


% ============================================================================
\subsection{Preprocessing}
\label{sec:preprocessing}

Each extracted character undergoes preprocessing to produce a vectorised representation, an OCR label, and a HOG descriptor.

\paragraph{Layout analysis and reading order.}
A projection-based layout module identifies columns and rows.
The reading order follows right-to-left, top-to-bottom (classical Chinese), assigning each character an integer index.

\paragraph{Vectorisation via affine scale-space.}
Binary patches are cleaned by morphological filtering (removing small holes and noise specks), then vectorised using affine scale-space smoothing~\cite{alvarez1993axioms,ciomaga2017curvature}.
This geometric PDE evolves each level line of the image according to its curvature:
\begin{equation}
    \frac{\partial u}{\partial t} = |\nabla u|\, \kappa^{1/3},
    \label{eq:gass}
\end{equation}
where $\kappa$ denotes the curvature of the level line through each point.
The $1/3$ exponent makes this evolution affine-invariant: the smoothed shape is independent of the viewing angle, which is essential for comparing characters printed under different conditions.
The resulting smooth level lines are converted to SVG vector outlines.
Document skew is estimated using LSD~\cite{von2010lsd} and corrected via the length-weighted circular mean of line segment orientations:
\begin{equation}
    \theta = \tfrac{1}{2}\arg\!\left(\textstyle\sum_i w_i \, e^{2\mathrm{i}\alpha_i}\right).
    \label{eq:skew}
\end{equation}

\paragraph{Character recognition (CHAT).}
Characters are labelled using a Kraken-based~\cite{kiessling2019kraken} model operating at the subcolumn level.
Predicted character sequences are spatially matched to CRAFT detections via the Hungarian algorithm on vertical distances.
Each character receives a label $\ell_i \in \mathcal{A} \cup \{\square\}$ and confidence $c_i \in [0,1]$.


% ---------- HOG ----------
\paragraph{HOG descriptors.}
Each character is described by a HOG descriptor~\cite{dalal2005histograms} computed on a rendered SVG image ($24 \times 24$\,px cells, 256\,DPI).
Gradients use Gaussian derivative filters ($\sigma = 5$, kernel size~31\,px).
Unsigned orientations are accumulated into $B = 16$ bins per cell via trilinear interpolation:
\begin{equation}
    h_b = \sum_{(x,y) \in \text{cell}} m(x,y) \cdot \bigl[(1 - \alpha)\,\delta_{b, \lfloor \hat\theta \rfloor} + \alpha\,\delta_{b, (\lfloor \hat\theta \rfloor + 1) \bmod B}\bigr],
\end{equation}
where $\hat\theta = \theta \cdot B$ and $\alpha = \hat\theta - \lfloor\hat\theta\rfloor$.
The descriptor is normalised via L2-clip-L2 with clipping threshold $t = 0.2$.

\input{figures/fig_hog_descriptor}


% ============================================================================
\subsection{A Contrario Feature Matching}
\label{sec:acontrario}

Character similarity is assessed using an \textit{a contrario} framework~\cite{desolneux2000meaningful} that provides a statistically grounded matching threshold.

\paragraph{Dissimilarity metric.}
Given two HOG descriptors $\mathbf{h}^A, \mathbf{h}^B$ with $K$ cells of $B$ bins each, we compute the Circular Earth Mover's Distance (CEMD) per cell.
For cumulative difference $X_j = \sum_{i=1}^{j} h^A_{k,i} - \sum_{i=1}^{j} h^B_{k,i}$:
\begin{equation}
    \text{CEMD}(h^A_k, h^B_k) = \min_{s \in \{0,\ldots,B{-}2\}} \frac{1}{B}\sum_{j=1}^{B} |X_j - X_s|.
    \label{eq:cemd}
\end{equation}
The total dissimilarity is $D(\mathbf{h}^A, \mathbf{h}^B) = \sum_{k=1}^{K} \text{CEMD}(h^A_k, h^B_k)$.

\paragraph{Number of False Alarms.}
Under the \textit{a contrario} null model, $D$ is approximately Gaussian with character-specific moments $(\mu_A, \sigma_A^2)$ estimated from all pairwise comparisons.
The NFA is:
\begin{equation}
    \text{NFA}(A,B) = N^2 \cdot \Phi\!\left(\frac{D(\mathbf{h}^A, \mathbf{h}^B) - \mu_A}{\sigma_A}\right),
    \label{eq:nfa}
\end{equation}
where $\Phi$ is the standard normal CDF.
Two characters are meaningfully similar when $\text{NFA} \le \varepsilon = 5 \times 10^{-4}$.
We store the negative log-NFA: $\text{NLFA}(A,B) = -\log\Phi\!\bigl(\frac{D - \mu_A}{\sigma_A}\bigr)$.


% ============================================================================
\subsection{Graph Construction and Community Detection}
\label{sec:graph-clustering}

A similarity graph $G = (V, E, w)$ is built with one vertex per character.
An edge $(i,j)$ is created when both NLFA values exceed threshold $\tau_\text{NFA} = -\log\varepsilon + 2\log N$:
\begin{equation}
    (i,j) \in E \iff \text{NLFA}(i,j) \ge \tau_\text{NFA} \;\wedge\; \text{NLFA}(j,i) \ge \tau_\text{NFA}.
    \label{eq:reciprocal}
\end{equation}
The reciprocal condition discards asymmetric similarities.
Edge weights are the symmetrised NLFA: $w_{ij} = \frac{1}{2}[\text{NLFA}(i,j) + \text{NLFA}(j,i)]$.

Communities are detected using the Leiden algorithm~\cite{traag2019louvain} with the Reichardt--Bornholdt quality function:
\begin{equation}
    \mathcal{Q}_\gamma = \sum_{c} \left[e_c - \gamma \binom{n_c}{2}\right],
\end{equation}
where $e_c$ is the total edge weight within community $c$, $n_c$ its size, and $\gamma = 1.0$.

\input{figures/fig_clustering_pipeline}


% ============================================================================
\subsection{Cluster Refinement}
\label{sec:refinement}

The Leiden partition may contain impure or over-fragmented clusters.
A sequential three-stage refinement addresses these issues (Fig.~\ref{fig:refinement-pipeline}).

\paragraph{Stage~1: Hausdorff-based splitting.}
Large clusters ($\ge 5$ members) are inspected using pairwise Hausdorff distances~\cite{rony2025hausdorff} on registered binary images.
Registration uses multiscale inverse compositional (IC) alignment~\cite{briand2018ipol} with a Lorentzian robust error.
The Hausdorff distance matrix is used to build an average-linkage dendrogram, cut at $\tau_\text{split} = 21.5$.

\paragraph{Stage~2: OCR-based rematching.}
Small clusters ($\le 3$ members) are merged into the largest cluster sharing the same dominant OCR label.
This fast, label-driven step handles fragmented Leiden partitions.

\paragraph{Stage~3: PCA z-score rematching.}
Remaining small clusters are tested against $k=5$ nearest large clusters ($\ge 10$ members) via image-space PCA compatibility.
The query is registered against each candidate's most central member, projected into the candidate's PCA space ($d=5$ components), and merged if $\max_j |z_j| < 3.0$.

\input{figures/fig_refinement_pipeline}


% ============================================================================
\subsection{Evaluation Metrics}
\label{sec:metrics}

Clustering quality is evaluated using: Adjusted Rand Index (ARI)~\cite{hubert1985comparing} as the primary metric, Normalised Mutual Information (NMI), homogeneity/completeness/V-measure~\cite{rosenberg2007v}, pairwise F1 score, and Hungarian-matching accuracy.
Characters with unknown ground-truth labels are excluded from all metric computations.


% ============================================================================
\subsection{Glossary Construction}
\label{sec:glossary}

The final output of the pipeline is a \emph{character glossary}: for each cluster, we select the most central member (minimising mean intra-cluster dissimilarity) as the representative, and record the dominant OCR label, cluster size, and purity.
The glossary is sorted by frequency, producing an inventory of all distinct character forms in the book along with their vectorised outlines---the desired output of reverse typography.
