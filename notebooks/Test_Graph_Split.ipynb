{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba329fc6",
   "metadata": {},
   "source": [
    "# Cluster splitting schemes\n",
    "\n",
    "This is a temporary notebook, in which we investigate all the methods to split the clusters. More precisely, we look for:\n",
    "\n",
    "- Metrics telling us wether or not we should split a cluster\n",
    "- Metrics to tell us in how much clusters we should split it\n",
    "- Methods to split the clusters\n",
    "\n",
    "We will do a grid search to look for the splitting algorithms with the best overall performance, without worrying yet about explanations or interpreations. This will allow us to be way more efficient.\n",
    "\n",
    "\n",
    "\n",
    "Regarding the methods to split the clusters, we distinguish:\n",
    "- Methods which does not need an a priori number of clusters\n",
    "- Methods which needs an a priori number of clusters\n",
    "- Methods to split the clusters in two, which we can integrate into Diego's binary-tree like cluster splitting scheme.\n",
    "\n",
    "\n",
    "We begin with the methods who does not need an a priori number of clusters. These can be:\n",
    "- Methods that works on graphs, in which case we should decide wether we use the NLFA/filtered graph, the NLFA graph or dissimilarity graph.\n",
    "- Methods that works on features, in which case we can use either the image representation, the HOG, or other features.\n",
    "- Methods that works on metric spaces, allowing us to apply the registration algorithm when creating the clusters, to get more informative metrics.\n",
    "\n",
    "One of the biggest issues when doing clustering is to find an informative metric. In fact, when using high-dimensional vectors like the raw image pixels, or a precise HOG, the features all trend to be the same distance from one another, and the distances become less informative. Our assumption is that the extracted features lies on a low-dimensionnality manifold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81c4e1",
   "metadata": {},
   "source": [
    "# Imports & Data Loading\n",
    "\n",
    "We load all the data into two dataframes:\n",
    "- patches_df: all the info about all characters in the document\n",
    "- repr_df: The representative of the clusters, i.e. the character with the biggest number of connections\n",
    "- happax_df: the subset of patches_df containing all happax\n",
    "\n",
    "We also load the graph we constructed during the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d1ec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31203/2376845527.py:5: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../confs\", job_name=\"notebook\")\n",
      "/home/mathis-wauquiez/miniconda3/envs/projetOCR/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from hydra import initialize, compose\n",
    "initialize(config_path=\"../confs\", job_name=\"notebook\")\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "# We change the working directory to the root of the project\n",
    "# Run this only once\n",
    "root_path = Path.cwd().parent\n",
    "os.chdir(root_path)\n",
    "sys.path.append(root_path / \"src\")\n",
    "\n",
    "\n",
    "from src.utils import torch_to_pil\n",
    "from src.utils import connectedComponent\n",
    "from src.patch_processing.patch_extraction import extract_patches\n",
    "\n",
    "from notebook_utils.descriptor import compute_hog, visualize_hog\n",
    "\n",
    "from einops import rearrange\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "from notebook_utils.parquet_utils import load_dataframe, save_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84705844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading column: bin_patch\n",
      "Loading column: img_patch\n",
      "Loading column: page\n",
      "Loading column: file\n",
      "Loading column: left\n",
      "Loading column: top\n",
      "Loading column: width\n",
      "Loading column: height\n",
      "Loading column: label\n",
      "Loading column: page_skew\n",
      "Loading column: reading_order\n",
      "Loading column: svg\n",
      "Loading column: unc_qwen_single\n",
      "Loading column: char_qwen_single\n",
      "Loading column: histogram\n",
      "Loading column: mu_tot\n",
      "Loading column: var_tot\n",
      "Loading column: degree_centrality\n",
      "Loading column: membership_pre_split\n",
      "Loading column: membership\n",
      "Loading column: best_hog_config\n",
      "Loading column: best_cell_size\n",
      "Loading column: best_grdt_sigma\n",
      "Loading column: best_num_bins\n",
      "✓ Loaded from outputs/clustering/book1/clustered_patches\n",
      "Loading column: bin_patch\n",
      "Loading column: img_patch\n",
      "Loading column: page\n",
      "Loading column: file\n",
      "Loading column: left\n",
      "Loading column: top\n",
      "Loading column: width\n",
      "Loading column: height\n",
      "Loading column: label\n",
      "Loading column: page_skew\n",
      "Loading column: reading_order\n",
      "Loading column: svg\n",
      "Loading column: unc_qwen_single\n",
      "Loading column: char_qwen_single\n",
      "Loading column: histogram\n",
      "Loading column: mu_tot\n",
      "Loading column: var_tot\n",
      "Loading column: degree_centrality\n",
      "Loading column: membership_pre_split\n",
      "Loading column: membership\n",
      "Loading column: best_hog_config\n",
      "Loading column: best_cell_size\n",
      "Loading column: best_grdt_sigma\n",
      "Loading column: best_num_bins\n",
      "✓ Loaded from outputs/clustering/book1/filtered_patches\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(\"outputs/clustering/book1\")\n",
    "\n",
    "patches_df = load_dataframe(output_path / 'clustered_patches')\n",
    "repr_df = load_dataframe(output_path / 'filtered_patches')\n",
    "graph = pickle.load(open(output_path / 'graph.gpickle', mode='rb'))\n",
    "\n",
    "features = np.stack(patches_df['histogram'])\n",
    "features = features.reshape(len(features), -1)\n",
    "patches_df['histogram'] = list(features)\n",
    "membership = patches_df['membership']\n",
    "\n",
    "value, count = np.unique(membership, return_counts=True)\n",
    "happax_classes = value[count == 1]\n",
    "\n",
    "happax_df = patches_df[patches_df['membership'].isin(happax_classes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec767439",
   "metadata": {},
   "source": [
    "## Renderers & Source imports\n",
    "\n",
    "**Reminder**: renderers are the objects we use to convert the SVGs (``̀̀`patches_df['svg']``̀̀`) into plain pixels.\\\n",
    "**featureMatching** is the class to compute the NLFA & dissimilarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a192e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering: 100%|██████████| 13517/13517 [00:11<00:00, 1136.32img/s]\n"
     ]
    }
   ],
   "source": [
    "from src.patch_processing.hog import HOG\n",
    "from src.patch_processing.renderer import Renderer\n",
    "from src.patch_processing.params import HOGParameters\n",
    "from src.clustering.feature_matching import featureMatching\n",
    "from src.clustering.params import featureMatchingParameters\n",
    "\n",
    "\n",
    "renderer = Renderer(\n",
    "    scale=1.0,\n",
    "    dpi=256,\n",
    "    bin_thresh=0.5,\n",
    "    pad_to_multiple=24,\n",
    "    svg_imgs=patches_df['svg']\n",
    ")\n",
    "\n",
    "\n",
    "canvas_dims = renderer.canvas_dims\n",
    "barycenters = renderer.barycenters\n",
    "\n",
    "fMParams = featureMatchingParameters(metric=\"L2\", partial_output=False)\n",
    "featureMatcher = featureMatching(fMParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6680a7a0",
   "metadata": {},
   "source": [
    "_________\n",
    "## Computing metrics between registered images\n",
    "\n",
    "We investigate successively how:\n",
    "- a) to compute the metrics between two images using our class\n",
    "- b) much time does the alignment step take depending on the inverse compositional algorithm parameters and the renderer\n",
    "- c) much time does every step of computing the distances actually take\n",
    "- d) long does it take to compute the metrics inside one given cluster, depending on its size\n",
    "- e) much time it takes to compute the metrics between some patch inside the trash and all representatives\n",
    "- f) to be lazy and associate it the fastest\n",
    "  - We investigate the recall @ k vs computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e839dc",
   "metadata": {},
   "source": [
    "#### a) Registering and computing the metrics between two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d27f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registration Metrics:\n",
      "  hausdorff: 5.8310\n"
     ]
    }
   ],
   "source": [
    "# Test the metrics between registered images\n",
    "\n",
    "# from src.clustering.bin_image_metrics import reg_metric\n",
    "from src.clustering.bin_image_metrics import compute_hausdorff, registeredMetric, dice_coefficient, jaccard_index, compute_hamming\n",
    "\n",
    "metrics_dict = {\n",
    "    'hausdorff': compute_hausdorff,\n",
    "    # 'dice': dice_coefficient,\n",
    "    # 'jaccard': jaccard_index,\n",
    "    # 'hamming': compute_hamming,\n",
    "}\n",
    "\n",
    "reg_metric = registeredMetric(metrics=metrics_dict, sym=True)\n",
    "\n",
    "\n",
    "results = reg_metric(renderer[0], renderer[1]) # aligns with multiscale IC and computes metrics\n",
    "\n",
    "print(\"Registration Metrics:\")\n",
    "for metric_name, value in results.items():\n",
    "    print(f\"  {metric_name}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ceab2b",
   "metadata": {},
   "source": [
    "##### 1] between two random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c61c5e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5 ms ± 293 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "I1 = renderer[0].to('cuda').unsqueeze(0)\n",
    "I2 = renderer[1].to('cuda').unsqueeze(0)\n",
    "\n",
    "%timeit reg_metric.ic.run(I1, I2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f1f91",
   "metadata": {},
   "source": [
    "#### 2] between two similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aee4fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.3 ms ± 249 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "subdf = patches_df[patches_df['membership'] == 0]\n",
    "\n",
    "I1 = renderer[subdf.index[0]].to('cuda').unsqueeze(0)\n",
    "I2 = renderer[subdf.index[1]].to('cuda').unsqueeze(0)\n",
    "\n",
    "%timeit reg_metric.ic.run(I1, I2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca692b7",
   "metadata": {},
   "source": [
    "#### b) How much time does the alignment step take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ac1d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.registration.single_scale import InverseCompositional\n",
    "from src.registration.gradients import Gradients\n",
    "from src.registration.gaussian_pyramid import GaussianPyramid\n",
    "from src.registration.multiscale_registration import MultiscaleIC\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "gradient_method = Gradients(method='farid5', C=1, device=device)\n",
    "single_scale_ic = InverseCompositional(\n",
    "    transform_type='homography',\n",
    "    gradient_method=gradient_method,\n",
    "    error_function='lorentzian',\n",
    "    delta=5,\n",
    "    epsilon=1e-3,\n",
    "    max_iter=5,\n",
    ")\n",
    "\n",
    "gaussian_pyramid = GaussianPyramid(\n",
    "    eta=0.5,                        # unzooming factor\n",
    "    sigma_0=0.6,                    # initial std of the gaussian kernel\n",
    "    ksize_factor=8,                 # kernel size = 2 * sigma * ksize_factor | 1\n",
    "    min_size=32                     # size of the coarsest image in the pyramid\n",
    ")\n",
    "\n",
    "# Create the multiscale registration\n",
    "ic = MultiscaleIC(\n",
    "    singleScaleIC=single_scale_ic,\n",
    "    gaussianPyramid=gaussian_pyramid\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a37168",
   "metadata": {},
   "source": [
    "______\n",
    "**TO-DO**\n",
    "\n",
    "* Measure intra-cluster alignment time as a function of resolution\n",
    "* Measure happax alignment time\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8edb2",
   "metadata": {},
   "source": [
    "#### c) How much time does the distance computation take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdf9f862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44 ms ± 98 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "warped_bin = I2.squeeze() > 0.5\n",
    "I1_bin = I1.squeeze() > 0.5\n",
    "\n",
    "distances = {\n",
    "    metric_name: metric(I1_bin, warped_bin)\n",
    "    for metric_name, metric in reg_metric.metrics.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626776d",
   "metadata": {},
   "source": [
    "**Note that the registration is 10 times slower than the Hausdoff distance computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93b5bcc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlfa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m i = np.random.randint(\u001b[38;5;28mlen\u001b[39m(happax_df))\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# i = 3\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m topk_result = \u001b[43mnlfa\u001b[49m[i].topk(k=\u001b[32m5\u001b[39m)\n\u001b[32m     30\u001b[39m topk_best = topk_result.indices\n\u001b[32m     31\u001b[39m topk_values = topk_result.values\n",
      "\u001b[31mNameError\u001b[39m: name 'nlfa' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_nearest_match(featureMatcher, query_df, key_df):\n",
    "\n",
    "\n",
    "    def get_candidate_matches(featureMatcher, Q, K, topk=5):\n",
    "        _, nlfa, dissim = featureMatcher.match(query_histograms=Q, key_histograms=K)\n",
    "        topk_result = nlfa.topk(k=topk)\n",
    "        topk_indices = topk_result.indices\n",
    "        topk_values = topk_result.values\n",
    "        return topk_indices, topk_values\n",
    "\n",
    "\n",
    "    to_qk = lambda df: torch.tensor(np.stack(df['histogram'])).to('cuda')\n",
    "\n",
    "    K = to_qk(key_df)\n",
    "    Q= to_qk(query_df)\n",
    "\n",
    "    topk_indices, topk_values = get_candidate_matches(featureMatcher, Q, K)\n",
    "\n",
    "    return topk_indices, topk_values\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = np.random.randint(len(happax_df))\n",
    "\n",
    "# i = 3\n",
    "\n",
    "topk_result = nlfa[i].topk(k=5)\n",
    "topk_best = topk_result.indices\n",
    "topk_values = topk_result.values\n",
    "\n",
    "fig, axes = plt.subplots(1, 6, figsize=(18, 3))\n",
    "\n",
    "# Reference image\n",
    "axes[0].imshow(happax_df.iloc[i]['svg'].render())\n",
    "axes[0].set_title('Reference')\n",
    "axes[0].axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f937e7",
   "metadata": {},
   "source": [
    "#### d) How much time does it take to compute the distance matrices inside one cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd8c990",
   "metadata": {},
   "source": [
    "#### e) How much time does it take to compute the distances between one patch and all representatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d34916",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topk_best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Top-k matches\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (idx, val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(topk_best, topk_values)):\u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_metrics\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_data\u001b[39m(\n\u001b[32m      6\u001b[39m     community_id,\n\u001b[32m      7\u001b[39m     graph,\n\u001b[32m      8\u001b[39m     dataframe\n\u001b[32m      9\u001b[39m ):\n",
      "\u001b[31mNameError\u001b[39m: name 'topk_best' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Top-k matches\n",
    "for i, (idx, val) in enumerate(zip(topk_best, topk_values)):from joblib import Parallel, delayed\n",
    "from src.clustering.metrics import compute_metrics\n",
    "\n",
    "def extract_data(\n",
    "    community_id,\n",
    "    graph,\n",
    "    dataframe\n",
    "):\n",
    "    node_list = list(graph.nodes())\n",
    "    membership = dataframe['membership']\n",
    "\n",
    "    node_indices = np.where(membership == community_id)[0]\n",
    "    nodes_in_community = [node_list[i] for i in node_indices]\n",
    "    subgraph_nx = graph.subgraph(nodes_in_community).copy()\n",
    "\n",
    "    return dataframe.iloc[node_indices], subgraph_nx, node_indices\n",
    "\n",
    "def _process_community(community, graph, dataframe, splitting_method, min_size):\n",
    "    subdf, subgraph, node_indices = extract_data(community, graph, dataframe)\n",
    "    new_membership = splitting_method(subdf, subgraph, min_size=min_size)\n",
    "    \n",
    "    unique_memberships = np.unique(new_membership)\n",
    "    expected_range = np.arange(len(unique_memberships))\n",
    "    assert np.array_equal(unique_memberships, expected_range), \\\n",
    "        f\"Membership should be 0 to {len(unique_memberships)-1}, got {unique_memberships}\"\n",
    "    \n",
    "    return node_indices, new_membership, len(unique_memberships)\n",
    "\n",
    "def evaluate_splitting_method(\n",
    "    graph,\n",
    "    dataframe,\n",
    "    splitting_method,\n",
    "    min_size            = min_size,\n",
    "    membership_col_name = 'membership_split',\n",
    "    character_col_name = 'char_qwen_single',\n",
    "    use_tqdm=False,\n",
    "    n_jobs=-1\n",
    "):\n",
    "    membership = dataframe['membership']\n",
    "    communities = np.unique(membership)\n",
    "    \n",
    "    dataframe[membership_col_name] = -1\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs, verbose=10 if use_tqdm else 0)(\n",
    "        delayed(_process_community)(c, graph, dataframe, splitting_method, min_size)\n",
    "        for c in communities\n",
    "    )\n",
    "    \n",
    "    total_communities = 0\n",
    "    for node_indices, new_membership, n_unique in results:\n",
    "        dataframe.loc[node_indices, membership_col_name] = new_membership + total_communities\n",
    "        total_communities += n_unique\n",
    "\n",
    "    if use_tqdm:\n",
    "        print('Computing the metrics ...')\n",
    "    return compute_metrics(reference_labels=dataframe[character_col_name], predicted_labels=dataframe[membership_col_name], exclude_label=UNKNOWN_LABEL)\n",
    "    axes[i+1].imshow(repr_df['svg'].iloc[idx.item()].render())\n",
    "    axes[i+1].set_title(f'Top {i+1}\\n{val:.3f}')\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a6cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13517/13517 [02:50<00:00, 79.47it/s] \n"
     ]
    }
   ],
   "source": [
    "reg_metric = registeredMetric(metrics=metrics_dict, sym=True, lazy_metric=compute_hausdorff, lazy_threshold=24)\n",
    "\n",
    "I1 = renderer[happax_df.index[0]]\n",
    "\n",
    "for I2 in tqdm.tqdm(renderer):\n",
    "    reg_metric(I1, I2, sym=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dadfbda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.7 ms ± 678 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit reg_metric(renderer[0], renderer[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade6bff",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "# Splitting a cluster\n",
    "\n",
    "We successively investigate:\n",
    "- Splitting via agglomerative clustering on the Haursdoff distance\n",
    "- Other splitting methods\n",
    "- We vary the distance - using NLFA for example\n",
    "\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804fc346",
   "metadata": {},
   "source": [
    "### Step [a]: Computing the intra-cluster distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf1a6bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "568bf2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21609"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "147*147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8af4a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:17<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "cluster_id = 14\n",
    "subdf = patches_df[patches_df['membership_pre_split'] == cluster_id]\n",
    "\n",
    "from src.clustering.bin_image_metrics import compute_distance_matrices_batched\n",
    "\n",
    "distance_matrices = compute_distance_matrices_batched(\n",
    "    reg_metric, renderer, subdf,\n",
    "    batch_size=256,   # tune: 32 for small VRAM, 128+ for large\n",
    "    use_tqdm=True\n",
    ")\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import numpy as np\n",
    "\n",
    "D = distance_matrices['hausdorff']\n",
    "\n",
    "D[np.isnan(D)] = np.nanmax(D[~np.isnan(D)]) * 1.5\n",
    "\n",
    "svgs = subdf['svg']\n",
    "svgs_rendered = [svg.render(scale=0.3) for svg in svgs]\n",
    "\n",
    "condensed_D = squareform(D, checks=False)\n",
    "linkage_matrix = linkage(condensed_D, method='average')\n",
    "\n",
    "# Create LARGE figure with scrollable capability\n",
    "fig, ax = plt.subplots(figsize=(max(20, len(svgs) * 0.2), 10))\n",
    "\n",
    "dendro = dendrogram(linkage_matrix, ax=ax, no_labels=True)\n",
    "\n",
    "leaf_indices = dendro['leaves']\n",
    "leaf_x_positions = [5 + 10*i for i in range(len(leaf_indices))]\n",
    "\n",
    "# Get y-axis limits and extend MORE to accommodate staggered images\n",
    "y_min, y_max = ax.get_ylim()\n",
    "image_space = (y_max - y_min) * 0.5  # Increased from 0.3 to make more room\n",
    "ax.set_ylim(y_min - image_space, y_max)\n",
    "\n",
    "# Create alternating y positions (3 levels for better spacing)\n",
    "n_levels = 3  # Number of alternating levels\n",
    "base_y = y_min - image_space * 0.3\n",
    "level_spacing = image_space * 0.2  # Vertical spacing between levels\n",
    "\n",
    "# Add images with LARGER zoom for readability\n",
    "zoom = 0.5  # Increased from 0.3\n",
    "image_annotations = []\n",
    "\n",
    "for i, leaf_idx in enumerate(leaf_indices):\n",
    "    x_pos = leaf_x_positions[i]\n",
    "    \n",
    "    # Alternate y position based on index\n",
    "    level = i % n_levels\n",
    "    y_offset = base_y - (level * level_spacing)\n",
    "    \n",
    "    img_array = svgs_rendered[leaf_idx]\n",
    "    \n",
    "    imagebox = OffsetImage(img_array, zoom=zoom, cmap='gray')\n",
    "    ab = AnnotationBbox(imagebox, (x_pos, y_offset),\n",
    "                        frameon=True,\n",
    "                        box_alignment=(0.5, 0.5),\n",
    "                        pad=0.1,\n",
    "                        bboxprops=dict(edgecolor='gray', linewidth=0.5))\n",
    "    ax.add_artist(ab)\n",
    "    image_annotations.append(ab)\n",
    "    \n",
    "    # Draw a light line connecting image to its leaf position\n",
    "    ax.plot([x_pos, x_pos], [y_offset + image_space*0.1, y_min], \n",
    "            color='lightgray', linewidth=0.5, linestyle=':', alpha=0.5, zorder=0)\n",
    "\n",
    "# Threshold line\n",
    "threshold_y = 21.5\n",
    "threshold_line = ax.axhline(y=threshold_y, color='red', linestyle='--', \n",
    "                            linewidth=2, picker=5)\n",
    "\n",
    "x_lim = ax.get_xlim()\n",
    "cluster_text = ax.text(x_lim[1] * 0.02, threshold_y + 1, '', \n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                       fontsize=12)\n",
    "\n",
    "def update_clusters(threshold):\n",
    "    clusters = fcluster(linkage_matrix, threshold, criterion='distance')\n",
    "    n_clusters = len(np.unique(clusters))\n",
    "    \n",
    "    cluster_text.set_text(f'{n_clusters} clusters')\n",
    "    cluster_text.set_y(threshold + 1)\n",
    "    \n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, 20))  # More colors\n",
    "    \n",
    "    for i, leaf_idx in enumerate(leaf_indices):\n",
    "        cluster_id = clusters[leaf_idx] - 1\n",
    "        color = colors[cluster_id % len(colors)]\n",
    "        image_annotations[i].patch.set_edgecolor(color)\n",
    "        image_annotations[i].patch.set_linewidth(3)  # Thicker borders\n",
    "    \n",
    "    threshold_line.set_label(f'Threshold: {threshold:.1f}')\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "update_clusters(threshold_y)\n",
    "\n",
    "class DraggableLine:\n",
    "    def __init__(self, line, update_func):\n",
    "        self.line = line\n",
    "        self.update_func = update_func\n",
    "        self.press = None\n",
    "        self.connect()\n",
    "\n",
    "    def connect(self):\n",
    "        self.cidpress = self.line.figure.canvas.mpl_connect('button_press_event', self.on_press)\n",
    "        self.cidrelease = self.line.figure.canvas.mpl_connect('button_release_event', self.on_release)\n",
    "        self.cidmotion = self.line.figure.canvas.mpl_connect('motion_notify_event', self.on_motion)\n",
    "\n",
    "    def on_press(self, event):\n",
    "        if event.inaxes != self.line.axes:\n",
    "            return\n",
    "        contains, attrd = self.line.contains(event)\n",
    "        if not contains:\n",
    "            return\n",
    "        self.press = event.ydata\n",
    "\n",
    "    def on_motion(self, event):\n",
    "        if self.press is None or event.inaxes != self.line.axes:\n",
    "            return\n",
    "        \n",
    "        y_new = event.ydata\n",
    "        y_new = max(0, min(y_max, y_new))\n",
    "        self.line.set_ydata([y_new, y_new])\n",
    "        self.update_func(y_new)\n",
    "\n",
    "    def on_release(self, event):\n",
    "        self.press = None\n",
    "\n",
    "draggable = DraggableLine(threshold_line, update_clusters)\n",
    "\n",
    "ax.set_title('Hierarchical Clustering - Use pan/zoom tools below!', fontsize=14, pad=20)\n",
    "ax.set_xlabel('Samples')\n",
    "ax.set_ylabel('Distance')\n",
    "\n",
    "# Enable pan and zoom toolbar\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c314aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fe6631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patches_df['clustering_feature'] = patches_df['histogram']\n",
    "\n",
    "min_size = 20\n",
    "metric = \"euclidean\"\n",
    "\n",
    "# Mean-Shift clustering\n",
    "quantiles    = np.linspace(0, 1, 11)[1:-1]\n",
    "\n",
    "# DBSCAN\n",
    "db_epsilons  = np.linspace(0.5,10,10)\n",
    "\n",
    "leiden_gammas = np.linspace(0.5, 2.0, 10)\n",
    "\n",
    "\n",
    "from src.clustering.algorithms import get_algorithms\n",
    "\n",
    "# Get all algorithms\n",
    "algorithms = get_algorithms(\n",
    "    quantiles=quantiles,\n",
    "    db_epsilons=db_epsilons,\n",
    "    leiden_gammas=leiden_gammas,\n",
    "    optics_min_samples=[5, 10, 15], \n",
    "    metric=metric,\n",
    "    min_size=min_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24638331",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_splitting_method' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     51\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mComputing the metrics ...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m compute_metrics(reference_labels=dataframe[character_col_name], predicted_labels=dataframe[membership_col_name], exclude_label=UNKNOWN_LABEL)\n\u001b[32m     58\u001b[39m evaluate_splitting_method(\n\u001b[32m     59\u001b[39m     graph,\n\u001b[32m     60\u001b[39m     dataframe=patches_df,\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     splitting_method=\u001b[43mtest_splitting_method\u001b[49m,\n\u001b[32m     62\u001b[39m     use_tqdm=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     63\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'test_splitting_method' is not defined"
     ]
    }
   ],
   "source": [
    "from src.clustering.metrics import compute_metrics\n",
    "\n",
    "def extract_data(\n",
    "    community_id,\n",
    "    graph,\n",
    "    dataframe\n",
    "):\n",
    "    node_list = list(graph.nodes())\n",
    "    membership = dataframe['membership']\n",
    "\n",
    "    node_indices = np.where(membership == community_id)[0]\n",
    "    nodes_in_community = [node_list[i] for i in node_indices]\n",
    "    subgraph_nx = graph.subgraph(nodes_in_community).copy()\n",
    "\n",
    "    return dataframe.iloc[node_indices], subgraph_nx, node_indices\n",
    "\n",
    "def evaluate_splitting_method(\n",
    "    graph,\n",
    "    dataframe,\n",
    "    splitting_method,\n",
    "    min_size            = min_size,\n",
    "    membership_col_name = 'membership_split',\n",
    "    character_col_name = 'char_qwen_single',\n",
    "    use_tqdm=False\n",
    "):\n",
    "    membership = dataframe['membership']\n",
    "    communities = np.unique(membership)\n",
    "    if use_tqdm:\n",
    "        communities = tqdm.tqdm(communities, desc=\"Splitting communities...\", colour=\"magenta\")\n",
    "\n",
    "    total_communities = 0\n",
    "    \n",
    "    # Initialize the new column in the original dataframe\n",
    "    dataframe[membership_col_name] = -1\n",
    "\n",
    "    for community in communities:\n",
    "        subdf, subgraph, node_indices = extract_data(community, graph, dataframe)\n",
    "\n",
    "        new_membership = splitting_method(subdf, subgraph, min_size=min_size) # result MUST be of the form [|0, N-1|]\n",
    "                                                                              # otherwise won't work as expected but won't crash\n",
    "\n",
    "        unique_memberships = np.unique(new_membership)\n",
    "        expected_range = np.arange(len(unique_memberships))\n",
    "        assert np.array_equal(unique_memberships, expected_range), \\\n",
    "            f\"Membership should be 0 to {len(unique_memberships)-1}, got {unique_memberships}\"\n",
    "        \n",
    "        dataframe.loc[node_indices, membership_col_name] = new_membership + total_communities\n",
    "        total_communities += len(unique_memberships)\n",
    "\n",
    "    if use_tqdm:\n",
    "        print('Computing the metrics ...')\n",
    "    return compute_metrics(reference_labels=dataframe[character_col_name], predicted_labels=dataframe[membership_col_name], exclude_label=UNKNOWN_LABEL)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluate_splitting_method(\n",
    "    graph,\n",
    "    dataframe=patches_df,\n",
    "    splitting_method=test_splitting_method,\n",
    "    use_tqdm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from src.clustering.metrics import compute_metrics\n",
    "\n",
    "UNKNOWN_LABEL = '▯'\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from src.clustering.metrics import compute_metrics\n",
    "\n",
    "def extract_data(\n",
    "    community_id,\n",
    "    graph,\n",
    "    dataframe\n",
    "):\n",
    "    node_list = list(graph.nodes())\n",
    "    membership = dataframe['membership']\n",
    "\n",
    "    node_indices = np.where(membership == community_id)[0]\n",
    "    nodes_in_community = [node_list[i] for i in node_indices]\n",
    "    subgraph_nx = graph.subgraph(nodes_in_community).copy()\n",
    "\n",
    "    return dataframe.iloc[node_indices], subgraph_nx, node_indices\n",
    "\n",
    "def _process_community(community, graph, dataframe, splitting_method, min_size):\n",
    "    subdf, subgraph, node_indices = extract_data(community, graph, dataframe)\n",
    "    new_membership = splitting_method(subdf, subgraph, min_size=min_size)\n",
    "    \n",
    "    unique_memberships = np.unique(new_membership)\n",
    "    expected_range = np.arange(len(unique_memberships))\n",
    "    assert np.array_equal(unique_memberships, expected_range), \\\n",
    "        f\"Membership should be 0 to {len(unique_memberships)-1}, got {unique_memberships}\"\n",
    "    \n",
    "    return node_indices, new_membership, len(unique_memberships)\n",
    "\n",
    "def evaluate_splitting_method(\n",
    "    graph,\n",
    "    dataframe,\n",
    "    splitting_method,\n",
    "    min_size            = min_size,\n",
    "    membership_col_name = 'membership_split',\n",
    "    character_col_name = 'char_qwen_single',\n",
    "    use_tqdm=False,\n",
    "    n_jobs=-1\n",
    "):\n",
    "    membership = dataframe['membership']\n",
    "    communities = np.unique(membership)\n",
    "    \n",
    "    dataframe[membership_col_name] = -1\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs, verbose=10 if use_tqdm else 0)(\n",
    "        delayed(_process_community)(c, graph, dataframe, splitting_method, min_size)\n",
    "        for c in communities\n",
    "    )\n",
    "    \n",
    "    total_communities = 0\n",
    "    for node_indices, new_membership, n_unique in results:\n",
    "        dataframe.loc[node_indices, membership_col_name] = new_membership + total_communities\n",
    "        total_communities += n_unique\n",
    "\n",
    "    if use_tqdm:\n",
    "        print('Computing the metrics ...')\n",
    "    return compute_metrics(reference_labels=dataframe[character_col_name], predicted_labels=dataframe[membership_col_name], exclude_label=UNKNOWN_LABEL)\n",
    "\n",
    "\n",
    "def test_splitting_method(\n",
    "        subdf,\n",
    "        subgraph,\n",
    "        min_size=min_size\n",
    "):\n",
    "    return np.zeros_like(subdf['membership'])\n",
    "\n",
    "from functools import partial\n",
    "from src.clustering.algorithms import mean_shift\n",
    "evaluate_splitting_method(\n",
    "    graph,\n",
    "    dataframe=patches_df,\n",
    "    splitting_method=partial(mean_shift, q=0.3),\n",
    "    membership_col_name='mean_shift_0.3',\n",
    "    use_tqdm=True\n",
    ")\n",
    "\n",
    "# Rest remains unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0e183",
   "metadata": {},
   "source": [
    "Tested algorithms:\n",
    "- DBSCAN\n",
    "- HDBSCAN\n",
    "- Mean Shift\n",
    "- Leiden\n",
    "- Louvain\n",
    "- Label propagation\n",
    "- Greedy modularity\n",
    "\n",
    "To be added:\n",
    "- Same algorithms with different metrics\n",
    "- Others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c925941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils.evaluate_all_splitting import evaluate_all_algorithms, generate_report\n",
    "\n",
    "results_df, timings, failed_algos = evaluate_all_algorithms(\n",
    "    graph=graph,\n",
    "    dataframe=patches_df,\n",
    "    algorithms=algorithms, \n",
    "    character_col_name='char_qwen_single',\n",
    "    use_tqdm=True\n",
    ")\n",
    "\n",
    "# Generate complete report\n",
    "figs = generate_report(results_df, output_dir='./outputs/evaluation/clustering')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 54\n",
    "subdf = patches_df[patches_df['membership'] == i] \n",
    "while len(subdf) > 30 or len(np.unique(subdf['char_qwen_single'].values)) <= 2:\n",
    "    i += 1\n",
    "    subdf = patches_df[patches_df['membership'] == i] \n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5383f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_df['char_qwen_single']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.clustering.tsne_plot import plot_community_tsne\n",
    "\n",
    "\n",
    "i = 0\n",
    "subdf = patches_df[patches_df['membership'] == i] \n",
    "while len(subdf) > 30 or len(np.unique(subdf['char_qwen_single'].values)) <= 2:\n",
    "    i += 1\n",
    "    subdf = patches_df[patches_df['membership'] == i] \n",
    "i\n",
    "\n",
    "cluster_id = i\n",
    "\n",
    "\n",
    "# membership = label_propagation(\n",
    "#     subdf=patches_df[patches_df['membership'] == cluster_id],\n",
    "#     subgraph=None,\n",
    "#     # q=0.2,\n",
    "#     # min_size=10\n",
    "# )\n",
    "\n",
    "membership = patches_df['char_qwen_single']\n",
    "# membership = patches_df['membership_leiden_gamma=0.50']\n",
    "\n",
    "membership = membership[patches_df['membership'] == cluster_id]\n",
    "\n",
    "print('Total number of members: ', len(membership))\n",
    "\n",
    "\n",
    "unique, counts = np.unique(membership, return_counts=True)\n",
    "for val, count in zip(unique, counts):\n",
    "    print(f\"{val}: {count}\")\n",
    "\n",
    "_ = plot_community_tsne(\n",
    "    cluster_id=cluster_id,\n",
    "    color_by_membership=membership,\n",
    "    dataframe=patches_df,\n",
    "    graph=graph,\n",
    "    target_lbl='char_qwen_single',\n",
    "    disable_color=False,\n",
    "    disable_svg=False,\n",
    "    disable_char=False,\n",
    "    zoom=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.patch_processing.renderer import Renderer\n",
    "\n",
    "def compute_distance_matrices(subdf: pd.DataFrame, renderer: Renderer, reg_metric: dict):\n",
    "    \"\"\"\n",
    "    Compute pairwise metrics between all images in subdf.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    subdf : DataFrame\n",
    "        Subset of patches_df with images to compare\n",
    "    renderer : object\n",
    "        Renderer object that can index images\n",
    "    reg_metric : registeredMetric\n",
    "        Initialized metric calculator\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : {'metric_name': X}, with X the [N, N] matrix of distances\n",
    "    \"\"\"\n",
    "    indices = _, row1 in subdf.iterrows():subdf.index.tolist()\n",
    "    n = len(indices)\n",
    "    \n",
    "    # Initialize metric matrices\n",
    "    metric_names = list(reg_metric.metrics.keys())\n",
    "    metric_matrices = {name: np.zeros((n, n)) for name in metric_names}\n",
    "    \n",
    "    # Compute pairwise metrics\n",
    "    for i in tqdm(range(n), desc=\"Computing metrics\"):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                # Diagonal: perfect match\n",
    "                for name in metric_names:\n",
    "                    if name in ['dice', 'jaccard']:\n",
    "                        metric_matrices[name][i, j] = 1.0  # Perfect overlap\n",
    "                    elif name == 'hamming':\n",
    "                        metric_matrices[name][i, j] = 0.0  # No difference\n",
    "                    elif name == 'hausdorff':\n",
    "                        metric_matrices[name][i, j] = 0.0  # No distance\n",
    "            else:\n",
    "                # Compute metrics between different images\n",
    "                I1 = renderer[indices[i]]\n",
    "                I2 = renderer[indices[j]]\n",
    "                \n",
    "                results = reg_metric(I1, I2)\n",
    "                \n",
    "                for name in metric_names:\n",
    "                    metric_matrices[name][i, j] = results[name]\n",
    "    \n",
    "    return metric_matrices\n",
    "\n",
    "\n",
    "# Usage\n",
    "cluster_id = 1\n",
    "subdf = patches_df[patches_df['membership'] == cluster_id]\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the distance matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (metric_name, matrix) in enumerate(metric_matrices.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    im = ax.imshow(matrix, cmap='viridis', aspect='auto')\n",
    "    ax.set_title(f'{metric_name.capitalize()} Distance Matrix', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Image Index')\n",
    "    ax.set_ylabel('Image Index')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label(metric_name, rotation=270, labelpad=20)\n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'metric_matrices_cluster_{cluster_id}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Optional: Create a combined similarity score (normalized)\n",
    "def create_combined_score(metric_matrices):\n",
    "    \"\"\"\n",
    "    Create a combined similarity score (higher = more similar).\n",
    "    Normalizes all metrics to [0, 1] where 1 = most similar.\n",
    "    \"\"\"\n",
    "    n = metric_matrices['dice'].shape[0]\n",
    "    combined = np.zeros((n, n))\n",
    "    \n",
    "    # Dice and Jaccard: already in [0, 1], higher is better\n",
    "    combined += metric_matrices['dice']\n",
    "    combined += metric_matrices['jaccard']\n",
    "    \n",
    "    # Hamming: in [0, 1], lower is better → invert\n",
    "    combined += (1 - metric_matrices['hamming'])\n",
    "    \n",
    "    # Hausdorff: unbounded, lower is better → normalize and invert\n",
    "    hausdorff = metric_matrices['hausdorff'].copy()\n",
    "    hausdorff[np.isinf(hausdorff)] = np.nan\n",
    "    max_h = np.nanmax(hausdorff[~np.eye(n, dtype=bool)])\n",
    "    if max_h > 0:\n",
    "        hausdorff_norm = 1 - (hausdorff / max_h)\n",
    "        hausdorff_norm[np.isnan(hausdorff_norm)] = 0\n",
    "        combined += hausdorff_norm\n",
    "    \n",
    "    # Average across 4 metrics\n",
    "    combined /= 4.0\n",
    "    \n",
    "    return combined\n",
    "\n",
    "combined_similarity = create_combined_score(metric_matrices)\n",
    "\n",
    "# Plot combined similarity\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.imshow(combined_similarity, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "plt.title('Combined Similarity Score\\n(Higher = More Similar)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Image Index')\n",
    "plt.colorbar(label='Similarity Score')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'combined_similarity_cluster_{cluster_id}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projetOCR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
