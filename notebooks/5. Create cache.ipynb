{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f23670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21420/1901343020.py:5: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../confs\", job_name=\"notebook\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from hydra import initialize, compose\n",
    "initialize(config_path=\"../confs\", job_name=\"notebook\")\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "# We change the working directory to the root of the project\n",
    "# Run this only once\n",
    "root_path = Path.cwd().parent\n",
    "os.chdir(root_path)\n",
    "sys.path.append(root_path / \"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e66466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import torch_to_pil\n",
    "from src.character_linking.feature_matching import featureMatching\n",
    "from src.character_linking.params import HOGParameters, featureMatchingParameters, fullHOGOutput, featureMatchingOutputs\n",
    "from src.utils import connectedComponent\n",
    "from src.patch_processing.patch_extraction import extract_patches\n",
    "\n",
    "from notebook_utils.descriptor import compute_hog, visualize_hog\n",
    "\n",
    "from einops import rearrange\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f95fa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading column: bin_patch\n",
      "Loading column: img_patch\n",
      "Loading column: page\n",
      "Loading column: file\n",
      "Loading column: left\n",
      "Loading column: top\n",
      "Loading column: width\n",
      "Loading column: height\n",
      "Loading column: svg\n",
      "Loading column: aspect_ratio\n",
      "Loading column: predicted_char\n",
      "Loading column: histogram\n",
      "✓ Loaded from data/processed/book1_columnwise\n"
     ]
    }
   ],
   "source": [
    "from notebook_utils.parquet_utils import load_dataframe, save_dataframe\n",
    "\n",
    "patches_df = load_dataframe('data/processed/book1_columnwise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "977728fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 356962.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import threading\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "CACHE_PATH = \"deepseek_char_cache.json\"\n",
    "MAX_WORKERS = 6  # reduce if you hit rate limits\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load / initialize cache\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "_CACHE_LOCK = threading.Lock()\n",
    "\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    with open(CACHE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        _CHAR_CACHE = json.load(f)\n",
    "else:\n",
    "    _CHAR_CACHE = {}\n",
    "\n",
    "def _save_cache():\n",
    "    with open(CACHE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(_CHAR_CACHE, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# DeepSeek client\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-bc70ca511bb84365b9907211a59280cf\",\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Cached API call\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def get_traditional_chinese_character_meaning(char: str) -> str:\n",
    "    if not isinstance(char, str) or len(char) != 1:\n",
    "        raise ValueError(\"Input must be a single Chinese character.\")\n",
    "\n",
    "    # ---- Cache hit ----\n",
    "    with _CACHE_LOCK:\n",
    "        if char in _CHAR_CACHE:\n",
    "            return _CHAR_CACHE[char]\n",
    "\n",
    "    # ---- Cache miss → API call ----\n",
    "    prompt = (\n",
    "        \"Explain the meaning of the following Traditional Chinese character.\\n\\n\"\n",
    "        \"Requirements:\\n\"\n",
    "        \"- Assume the character is Traditional Chinese\\n\"\n",
    "        \"- Give the primary meaning\\n\"\n",
    "        \"- Mention common usages or contexts\\n\"\n",
    "        \"- Briefly note the etymology or radicals if relevant\\n\"\n",
    "        \"- Keep the explanation concise and precise\\n\\n\"\n",
    "        f\"Character: {char}\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a knowledgeable Chinese linguistics assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "\n",
    "    with _CACHE_LOCK:\n",
    "        _CHAR_CACHE[char] = answer\n",
    "        _save_cache()\n",
    "\n",
    "    return answer\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Cache warm-up over unique characters\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "unique_chars = np.unique(patches_df[\"predicted_char\"])\n",
    "\n",
    "def _safe_worker(char):\n",
    "    try:\n",
    "        get_traditional_chinese_character_meaning(char)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [\n",
    "        executor.submit(_safe_worker, char)\n",
    "        for char in unique_chars\n",
    "        if char not in _CHAR_CACHE  # skip cached\n",
    "    ]\n",
    "\n",
    "    for _ in tqdm(as_completed(futures), total=len(futures)):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ea102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projetOCR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
