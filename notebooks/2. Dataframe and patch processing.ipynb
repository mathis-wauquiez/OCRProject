{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82cef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157460/1901343020.py:5: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../confs\", job_name=\"notebook\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from hydra import initialize, compose\n",
    "initialize(config_path=\"../confs\", job_name=\"notebook\")\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "# We change the working directory to the root of the project\n",
    "# Run this only once\n",
    "root_path = Path.cwd().parent\n",
    "os.chdir(root_path)\n",
    "sys.path.append(root_path / \"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff857b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathis/anaconda3/envs/projetOCR/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.utils import torch_to_pil\n",
    "# from src.character_linking.feature_matching import featureMatching\n",
    "# from src.character_linking.params import HOGParameters, featureMatchingParameters, fullHOGOutput, featureMatchingOutputs\n",
    "from src.utils import connectedComponent\n",
    "from src.patch_processing.patch_extraction import extract_patches\n",
    "\n",
    "# from notebook_utils.viz import show_random_sample, savefig\n",
    "# from notebook_utils.descriptor import compute_hog, visualize_hog\n",
    "from einops import rearrange\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "image_folder = Path('data/datasets/book_small')\n",
    "# comps_folder = Path('outputs/extraction/book1/components/')\n",
    "comps_folder = Path('outputs/book_small/components/')\n",
    "\n",
    "\n",
    "assert image_folder.exists()\n",
    "assert comps_folder.exists()\n",
    "files = next(os.walk(image_folder))[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29ec0a",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "This notebook presents the pipeline used to preprocess the images and compute character-wise features. We:\n",
    "\n",
    "1. Preprocess the images using the Binary Vectorization Method by He. et al\n",
    "2. Render them into a grid, aligning the barycenters with the middle of the canvas\n",
    "3. Compute their Histogram of Oriented Gradients (HOG)\n",
    "4. Predict the character using EasyOCR's Traditional Chinese OCR\n",
    "5. Describe the meaning of the character using ChatGPT\n",
    "\n",
    "---\n",
    "\n",
    "# Part one: Pre-processing\n",
    "\n",
    "We begin by extracting image patches of individual characters from the source book and applying a series of pre-processing steps. First, the patches are vectorized using **Binary Shape Vectorization by Affine Scale-Space**, a method developed by Yuchen He.\n",
    "\n",
    "This technique converts binary input images into smooth SVG representations composed of Bézier curves. As a result, we obtain cleaner and smoother hànzì shapes, free from noise and JPEG compression artifacts.\n",
    "\n",
    "Subsequently, we render them at a higher resolution and apply additional filtering steps to remove ink defects and spurious artifacts within the character shapes.\n",
    "\n",
    "We now proceed by extracting the character patches and their corresponding binarized images from the document. Note that the binarized images are directly taken from the outputs of the hànzì extraction notebook. Regions predicted as not belonging to the actual character are excluded from the displayed patches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a722395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:04<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "patches_df = pd.DataFrame(columns=['bin_patch', 'img_patch', 'page', 'file', 'left', 'top', 'width', 'height', 'label'])\n",
    "\n",
    "for i, file in tqdm.tqdm(list(enumerate(files))):\n",
    "    # Load the image / components\n",
    "    img_np = np.array(Image.open(image_folder / file))\n",
    "    img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)[..., None]\n",
    "    # img_torch = torch.tensor(img_np, device=patches_device, dtype=patches_dtype).permute(2,0,1).float() / 255\n",
    "    # img_torch.requires_grad = False\n",
    "    img_comp = connectedComponent.load(comps_folder / (str(file) + '.npz'))\n",
    "    img_comp._stats = img_comp._compute_stats_from_labels(img_comp._labels)\n",
    "    \n",
    "    _bin_patches, _img_patches = extract_patches(\n",
    "        characterComponents=img_comp,\n",
    "        images = [img_np],\n",
    "        return_bin=True\n",
    "    )\n",
    "\n",
    "    lbls = [region.label for region in img_comp.regions]\n",
    "    lbls = list(filter(lambda x: not img_comp.is_deleted(x), lbls))\n",
    "\n",
    "\n",
    "    stats = img_comp.stats[1:]\n",
    "\n",
    "    page_df = pd.DataFrame({\n",
    "        'bin_patch': _bin_patches,\n",
    "        'img_patch': _img_patches,\n",
    "        'page': i,\n",
    "        'file': file,\n",
    "        'left': stats[:,0],\n",
    "        'top': stats[:, 1],\n",
    "        'width': stats[:,2],\n",
    "        'height': stats[:, 3],\n",
    "        'label': lbls\n",
    "    })\n",
    "    \n",
    "    # Concatenate immediately\n",
    "    patches_df = pd.concat([patches_df, page_df], ignore_index=True)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd5bfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3510, 4799, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98ac1cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function createLineSegmentDetector:\n",
      "\n",
      "createLineSegmentDetector(...)\n",
      "    createLineSegmentDetector([, refine[, scale[, sigma_scale[, quant[, ang_th[, log_eps[, density_th[, n_bins]]]]]]]]) -> retval\n",
      "    .   @brief Creates a smart pointer to a LineSegmentDetector object and initializes it.\n",
      "    .   \n",
      "    .   The LineSegmentDetector algorithm is defined using the standard values. Only advanced users may want\n",
      "    .   to edit those, as to tailor it for their own application.\n",
      "    .   \n",
      "    .   @param refine The way found lines will be refined, see #LineSegmentDetectorModes\n",
      "    .   @param scale The scale of the image that will be used to find the lines. Range (0..1].\n",
      "    .   @param sigma_scale Sigma for Gaussian filter. It is computed as sigma = sigma_scale/scale.\n",
      "    .   @param quant Bound to the quantization error on the gradient norm.\n",
      "    .   @param ang_th Gradient angle tolerance in degrees.\n",
      "    .   @param log_eps Detection threshold: -log10(NFA) \\> log_eps. Used only when advance refinement is chosen.\n",
      "    .   @param density_th Minimal density of aligned region points in the enclosing rectangle.\n",
      "    .   @param n_bins Number of bins in pseudo-ordering of gradient modulus.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.createLineSegmentDetector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e676e5",
   "metadata": {},
   "source": [
    "```text\n",
    "Help on built-in function detect:\n",
    "\n",
    "detect(...) method of cv2.LineSegmentDetector instance\n",
    "    detect(image[, lines[, width[, prec[, nfa]]]]) -> lines, width, prec, nfa\n",
    "    .   @brief Finds lines in the input image.\n",
    "    .   \n",
    "    .       This is the output of the default parameters of the algorithm on the above shown image.\n",
    "    .   \n",
    "    .       ![image](pics/building_lsd.png)\n",
    "    .   \n",
    "    .       @param image A grayscale (CV_8UC1) input image. If only a roi needs to be selected, use:\n",
    "    .       `lsd_ptr-\\>detect(image(roi), lines, ...); lines += Scalar(roi.x, roi.y, roi.x, roi.y);`\n",
    "    .       @param lines A vector of Vec4f elements specifying the beginning and ending point of a line. Where\n",
    "    .       Vec4f is (x1, y1, x2, y2), point 1 is the start, point 2 - end. Returned lines are strictly\n",
    "    .       oriented depending on the gradient.\n",
    "    .       @param width Vector of widths of the regions, where the lines are found. E.g. Width of line.\n",
    "    .       @param prec Vector of precisions with which the lines are found.\n",
    "    .       @param nfa Vector containing number of false alarms in the line region, with precision of 10%. The\n",
    "    .       bigger the value, logarithmically better the detection.\n",
    "    .       - -1 corresponds to 10 mean false alarms\n",
    "    .       - 0 corresponds to 1 mean false alarm\n",
    "    .       - 1 corresponds to 0.1 mean false alarms\n",
    "    .       This vector will be calculated only when the objects type is #LSD_REFINE_ADV.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61be74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001957219\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def get_document_orientation(\n",
    "        np_img:          np.ndarray,\n",
    "        min_line_length: float = 100,\n",
    "        atol:            float = np.pi/8\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Computes the document orientation using the lines detected by the Line Segment Detector.\n",
    "    Args:\n",
    "        np_img:          The input image\n",
    "        min_line_length: The minimum line length to utilize the line\n",
    "        atol:            The absolute tolerance to detect vertical / horizontal lines\n",
    "    Returns:\n",
    "        theta: float, document orientation in radians\n",
    "\n",
    "    Methodology:\n",
    "        - Compute the line segments using the Advanced refinement\n",
    "        --> cv2 doc: \"Number of false alarms is calculated, lines are refined through increase of precision, decrement in size, etc\"\n",
    "        - Only keep long lines\n",
    "        - Filter them by orientation (keep vertical/horizontal lines)\n",
    "        - Document orientation = mean of line orientations, weighted by line length\n",
    "    \"\"\"\n",
    "\n",
    "    # Detect lines\n",
    "    lsd = cv2.createLineSegmentDetector(cv2.LSD_REFINE_ADV)\n",
    "    lines, width, prec, nfa = lsd.detect(img_np[...,0])\n",
    "\n",
    "\n",
    "    dx = lines[..., 2]-lines[...,0]\n",
    "    dy = lines[..., 3]-lines[...,1]\n",
    "\n",
    "    lengths = (dy**2 + dx**2)**.5\n",
    "    theta = np.arctan2(dx, dy) % np.pi\n",
    "\n",
    "    # Keep big lines only\n",
    "    mask = lengths > min_line_length\n",
    "    theta = theta[mask]\n",
    "    lengths = lengths[mask]\n",
    "\n",
    "    # Filter by orientation\n",
    "    oris = (0, np.pi/2, np.pi)\n",
    "\n",
    "    lines_sum = 0\n",
    "    for ori in oris:\n",
    "        mask = np.isclose(theta, ori, atol=atol)\n",
    "        lines_sum += np.sum(lengths[mask] * np.exp(2j * (theta[mask]-ori)))\n",
    "\n",
    "    document_orientation = np.angle(lines_sum)\n",
    "    return document_orientation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17ad60bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'filter_binary_patch' from 'src.patch_processing' (/home/mathis/Bureau/OCRProject/src/patch_processing/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m filtered_images = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpatch_processing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m filter_binary_patch\n\u001b[32m      5\u001b[39m min_size_black=\u001b[32m75\u001b[39m,\n\u001b[32m      6\u001b[39m min_size_white=\u001b[32m20\u001b[39m,\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'filter_binary_patch' from 'src.patch_processing' (/home/mathis/Bureau/OCRProject/src/patch_processing/__init__.py)"
     ]
    }
   ],
   "source": [
    "filtered_images = []\n",
    "\n",
    "from src.patch_processing import filter_binary_patch\n",
    "\n",
    "min_size_black=75,\n",
    "min_size_white=20,\n",
    "\n",
    "for bin_patch in tqdm.tqdm(patches_df['bin_patch']):\n",
    "    #? Filter\n",
    "    filtered = filter_binary_patch(\n",
    "        bin_patch, \n",
    "        min_size_black=min_size_black, \n",
    "        min_size_white=min_size_white, \n",
    "    )\n",
    "\n",
    "    filtered_images.append(filtered)\n",
    "\n",
    "patches_df['filtered_bin_patch'] = filtered_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdadc8b",
   "metadata": {},
   "source": [
    "**We now proceed to vectorizing the hànzi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d17275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 17:21:09,613 - INFO - Initialized vectorizer:\n",
      "2026-01-13 17:21:09,613 - INFO -   output_size: (512, 512)\n",
      "2026-01-13 17:21:09,614 - INFO -   background_color: (255, 255, 255, 255)\n",
      "2026-01-13 17:21:09,614 - INFO -   output_format: L\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "_target_: src.vectorization.BinaryShapeVectorizer\n",
       "config:\n",
       "  _target_: src.vectorization.VectorizerConfig\n",
       "  executable_path: ./build/main\n",
       "  smoothing_scale: 0.6\n",
       "  accuracy_threshold: 1\n",
       "  refinement_iterations: 0\n",
       "  output_type: shape_merged\n",
       "  return_svg: true\n",
       "  return_svg_string: false\n",
       "  return_rendered: false\n",
       "  save_dir: ./outputs_svg\n",
       "  output_size:\n",
       "  - 512\n",
       "  - 512\n",
       "  background_color:\n",
       "  - 255\n",
       "  - 255\n",
       "  - 255\n",
       "  - 255\n",
       "  dpi: 2048\n",
       "  scale: 1.0\n",
       "  output_format: L\n",
       "  n_jobs: -1\n",
       "  show_progress: true\n",
       "  chunk_size: 100\n",
       "  stream_results: true\n",
       "  stream_parallel: true\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 17:21:09,653 - INFO - Processing 1486 binarized images\n",
      "2026-01-13 17:21:09,654 - INFO - First image shape: (115, 129), dtype: bool\n",
      "Vectorizing images: 100%|\u001b[32m██████████\u001b[0m| 1486/1486 [00:26<00:00, 56.57img/s]\n",
      "Computing bounding boxes: 100%|\u001b[34m██████████\u001b[0m| 1486/1486 [00:00<00:00, 43316.88svg/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image with max aspect ratio:  1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from hydra.utils import instantiate\n",
    "from operator import itemgetter\n",
    "from omegaconf import OmegaConf\n",
    "from src.patch_processing.normalization import compute_normalization_homography, compute_svg_normalization_homography\n",
    "from src.patch_processing import filter_binary_patch\n",
    "from IPython.display import SVG as IPYSVG\n",
    "from tqdm import tqdm\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
    "\n",
    "from src.patch_processing.svg import SVG\n",
    "\n",
    "# ==== Instantiate the vectorizer ====\n",
    "\n",
    "#? See configs/vectorizer.yaml\n",
    "cfg = compose(config_name=\"vectorizer\")\n",
    "vect = instantiate(cfg)\n",
    "\n",
    "#? We show the content of the config \n",
    "display(Markdown(f\"```yaml\\n{OmegaConf.to_yaml(cfg)}\\n```\"))\n",
    "\n",
    "\n",
    "#! ==== CACHING ====\n",
    "\n",
    "if False:\n",
    "    svg_imgs = []\n",
    "    for i in tqdm(range(len(patches_df)), desc=\"Loading vectorized images\", unit=\"img\", colour='green'):\n",
    "        svg_imgs.append(SVG.load(Path(cfg.config.save_dir) / f'output_{i:06d}.svg'))\n",
    "\n",
    "#! ==== Vectorize the images ===\n",
    "\n",
    "else:\n",
    "    inverted = [p<.5 for p in patches_df['filtered_bin_patch']]\n",
    "    svg_imgs = []\n",
    "    for svg in tqdm(vect(inverted), total=len(inverted), desc=\"Vectorizing images\", unit=\"img\", colour='green'): # <-- iterator over the images | gathers data in parallel and yields it\n",
    "        svg_imgs.append(svg)\n",
    "\n",
    "    del inverted # free some memory\n",
    "\n",
    "    svg_imgs = sorted(svg_imgs, key=itemgetter(0))\n",
    "    svg_imgs = [el[1] for el in svg_imgs]\n",
    "\n",
    "patches_df['svg'] = svg_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a92ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering: 100%|██████████| 1486/1486 [00:05<00:00, 265.81img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canvas dimensions: w=168, h=192, cx=86, cy=91\n",
      "Canvas size: 168×192 pixels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 17:33:13,131 - WARNING - /tmp/ipykernel_6075/772028763.py:208: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('report_figures/canvas/fig-000008.png')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.patch_processing.normalization import compute_moment\n",
    "from src.patch_processing import filter_binary_patch\n",
    "from torch.utils.data import Dataset\n",
    "from src.patch_processing.renderer import Renderer\n",
    "\n",
    "svg_imgs = patches_df['svg']\n",
    "\n",
    "dpi = 256\n",
    "scale = 1\n",
    "\n",
    "dataset = Renderer(\n",
    "    svg_imgs=patches_df['svg'],\n",
    "    scale=scale,\n",
    "    dpi=dpi,\n",
    "    bin_thresh=128,\n",
    "    pad_to_multiple=24\n",
    ")\n",
    "\n",
    "# Access canvas dimensions\n",
    "canvas_width = dataset.canvas_width\n",
    "canvas_height = dataset.canvas_height\n",
    "center_x = dataset.center_x\n",
    "center_y = dataset.center_y\n",
    "\n",
    "print(f\"Canvas size: {canvas_width}×{canvas_height} pixels\")\n",
    "\n",
    "# Create DataLoader for batching\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Set to >0 if rendering is CPU-intensive\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# Or get individual images\n",
    "N = 10\n",
    "fig, axes = plt.subplots(N, N, figsize=(24, 24), constrained_layout=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.axis('off')\n",
    "    \n",
    "    img = dataset[i].numpy()  # Get i-th image\n",
    "    ax.imshow(img, cmap='gray', vmin=0, vmax=1, aspect='equal')\n",
    "    ax.set_title(f'Image {i}', fontsize=10)\n",
    "    \n",
    "    ax.plot(center_x, center_y, 'r+', markersize=10, markeredgewidth=1.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "savefig(fig, 'canvas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3892761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathis/anaconda3/envs/projetOCR/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-13 17:25:43,214 - INFO - use model: /home/mathis/.cnocr/2.3/densenet_lite_136-gru/cnocr-v2.3-densenet_lite_136-gru-epoch=004-ft-model.onnx\n",
      "OCR Processing: 100%|██████████| 1486/1486 [05:11<00:00,  4.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.ocr.wrappers import available_wrappers, EnsembleOCR\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# initialize wrappers and ensemble\n",
    "wrappers = available_wrappers()              # dict[name -> model]\n",
    "ensemble = EnsembleOCR(wrappers)\n",
    "\n",
    "# prepare columns in the dataframe\n",
    "patches_df[\"ensemble_pred\"] = \"\"\n",
    "patches_df[\"ensemble_uncertainty\"] = 0.0\n",
    "patches_df[\"ensemble_probs\"] = None          # dict: char -> prob\n",
    "patches_df[\"individual_preds\"] = None        # dict: model -> {char, score, uncertainty}\n",
    "\n",
    "for idx in tqdm(range(len(patches_df)), desc=\"OCR Processing\"):\n",
    "    patch = patches_df[\"svg\"].iloc[idx].render(output_format=\"L\")\n",
    "\n",
    "    results = ensemble.predict([patch])\n",
    "\n",
    "    # ----- ensemble output -----\n",
    "    ens = results[\"ensemble\"][0]\n",
    "\n",
    "    patches_df.at[idx, \"ensemble_pred\"] = ens[\"char\"]\n",
    "    patches_df.at[idx, \"ensemble_uncertainty\"] = ens[\"uncertainty\"]\n",
    "    patches_df.at[idx, \"ensemble_probs\"] = ens[\"probs\"]\n",
    "\n",
    "    # ----- individual outputs -----\n",
    "    indiv_logged = {}\n",
    "\n",
    "    for model_name, preds in results[\"individual\"].items():\n",
    "        p = preds[0]\n",
    "        indiv_logged[model_name] = {\n",
    "            \"char\": p[\"char\"],\n",
    "            \"score\": p[\"score\"],\n",
    "            \"uncertainty\": 1.0 - p[\"score\"],\n",
    "        }\n",
    "\n",
    "    patches_df.at[idx, \"individual_preds\"] = indiv_logged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e970a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "\n",
    "# import notebook_utils.parquet_utils as pu\n",
    "# importlib.reload(pu)\n",
    "# from notebook_utils.parquet_utils import load_columns\n",
    "\n",
    "# ocr_columns = ['ensemble_pred', 'ensemble_uncertainty', 'ensemble_probs', 'individual_preds', 'predicted_char']\n",
    "# ocr_data = load_columns('data/tmp/svg', ocr_columns)\n",
    "\n",
    "# for col in ocr_columns:\n",
    "#     patches_df[col] = ocr_data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd0126e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing OCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OCR Processing: 100%|██████████| 1486/1486 [01:13<00:00, 20.31it/s]\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "reader = easyocr.Reader(['ch_tra'], gpu=True, verbose=False)\n",
    "\n",
    "print(\"Processing OCR...\")\n",
    "all_results = []\n",
    "\n",
    "for idx in tqdm(range(len(patches_df)), desc=\"OCR Processing\"):\n",
    "    patch = patches_df['svg'].iloc[idx].render(output_format='RGB')\n",
    "    if patch.dtype != np.uint8:\n",
    "        patch = patch.astype(np.uint8)\n",
    "    \n",
    "    try:\n",
    "        result = reader.readtext(patch, detail=0)\n",
    "        all_results.append(result[0] if result else '')\n",
    "    except:\n",
    "        all_results.append('')\n",
    "\n",
    "patches_df['predicted_char'] = all_results\n",
    "\n",
    "del reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2a3cb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving column: bin_patch\n",
      "Saving column: img_patch\n",
      "Saving column: page\n",
      "Saving column: file\n",
      "Saving column: left\n",
      "Saving column: top\n",
      "Saving column: width\n",
      "Saving column: height\n",
      "Saving column: label\n",
      "Saving column: filtered_bin_patch\n",
      "Saving column: ensemble_pred\n",
      "Saving column: ensemble_uncertainty\n",
      "Saving column: ensemble_probs\n",
      "Saving column: individual_preds\n",
      "Saving column: predicted_char\n",
      "✓ Saved to data/tmp/svg\n"
     ]
    }
   ],
   "source": [
    "from notebook_utils.parquet_utils import save_dataframe\n",
    "save_dataframe(patches_df.drop(columns='svg'), 'data/tmp/svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "035d97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from notebook_utils.parquet_utils import load_dataframe\n",
    "# from src.patch_processing.svg import SVG\n",
    "\n",
    "# patches_df = load_dataframe('data/tmp/svg')\n",
    "# patches_df['svg'] = patches_df['svg_str'].map(lambda s: SVG.load_from_string(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b7239",
   "metadata": {},
   "source": [
    "## Computing the HOG descriptor\n",
    "\n",
    "We now explain how we compute the HOG descriptor to match the different patches to characters.\n",
    "\n",
    "The computation is done as follows:\n",
    "\n",
    "1. Preprocessing step\n",
    "\n",
    "The images on which we wish to compute the descriptors are preprocessed.\n",
    "It is possible to compute the descriptors on either grayscale images or multi-channel images. We use grayscale conversion.\n",
    "We also resize the images to a fixed size.\n",
    "\n",
    "2. Computing the gradients\n",
    "\n",
    "There are several ways to compute the gradients. We first begin by using a gaussian filter to smooth the image.\n",
    "Then, we apply a separable convolution based on cv2.getDerivKernels to compute the image gradients. The two components of this separable convolutions are:\n",
    "- A smoothing operator on one axis\n",
    "- It's derivative on the other axis\n",
    "\n",
    "This is justified because of derivative operations on convolution products.:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x}(I * G) = I * \\frac{\\partial G}{\\partial x}$$\n",
    "\n",
    "For a 2D Gaussian $G(x,y) = G_x(x) \\cdot G_y(y)$, the x-gradient becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x}(I * G) = I * \\left(\\frac{\\partial G_x}{\\partial x} \\otimes G_y\\right)\n",
    "$$\n",
    "\n",
    "where $\\otimes$ denotes separable convolution: first convolve rows with $G_y$ (smooth), then columns with $\\frac{\\partial G_x}{\\partial x}$ (differentiate).\n",
    "\n",
    "3. Histogram Computation\n",
    "\n",
    "We begin by computing:\n",
    "- The orientation of the gradients in the image: $\\theta = \\text{arctan2}(dx, dy)$\n",
    "- Their magnitude: $M = \\sqrt{{dx}^2 + {dy}^2}$\n",
    "\n",
    "We rearrange these arrays of shape $(H, W)$ to $(\\text{Nh}, \\text{Nw}, h, w)$, with Nh and Nw the number of rows / cols of cells, $h$ and $w$ their height and width\n",
    "\n",
    "We will use trilinear interpolation to compute the histograms.\n",
    "For each pixel $u$ of the cell, we compute the two bins the cell should contribute to:\n",
    "\n",
    "$$i_1 = \\lfloor\\theta_u \\cdot N_{\\text{bins}}\\rfloor \\quad ; \\quad i_2 = i_1 + 1$$\n",
    "\n",
    "The contribution weights for angular interpolation are:\n",
    "$$w_2 = \\theta_u \\cdot N_{\\text{bins}} - i_1 \\quad ; \\quad w_1 = 1 - w_2$$\n",
    "\n",
    "Optionally, a Gaussian spatial weighting can be applied (used in SIFT, but not in standard HOG):\n",
    "$$w_{\\text{spatial}}(x, y) = \\exp\\left(-\\frac{x_n^2 + y_n^2}{2\\sigma^2}\\right)$$\n",
    "where $(x_n, y_n) \\in [-1, 1]^2$ are normalized coordinates from the cell center.\n",
    "\n",
    "Each pixel contributes to the histogram:\n",
    "$$H[i_1] \\mathrel{+}= M_u \\cdot w_1 \\cdot w_{\\text{spatial}} \\quad ; \\quad H[i_2] \\mathrel{+}= M_u \\cdot w_2 \\cdot w_{\\text{spatial}}$$\n",
    "\n",
    "4. Normalization\n",
    "\n",
    "As in SIFT, the descriptors can be optionally normalized.\n",
    "We can normalize them:\n",
    "- At cell-level: normalize each histogram to be unit norm (as done in SIFT)\n",
    "- At patch-level: normalize the vector of concatenated\n",
    "\n",
    "[This paper](https://www.ipol.im/pub/art/2014/82/article.pdf), Anatomy of the SIFT method, states that cell-level normalization can help with lightning changes.\n",
    "We don't have such lightning changes, and have small cells. We therefore prefeer to use patch-level normalization. \n",
    "\n",
    "As in SIFT, we clip the values of the descriptors to be in the $[-0.2, 0.2]$ range, before re-normalizing the descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dfbecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_params = HOGParameters(\n",
    "    device          = \"cuda\",\n",
    "    C               = 1,                        # Use grayscale images\n",
    "    partial_output  = False,                    # Also output the resized images, their gradient orientation and magnitude\n",
    "    method          = 'gaussian',               # Use gaussian smoothing to compute the gradients\n",
    "    grdt_sigma      = 5,                      # Std of the smoothing\n",
    "    ksize_factor    = 6,                        # Size of the smoothing kernel = factor * std\n",
    "    cell_height     = 24,                       # Size of the cells to compute the histograms\n",
    "    cell_width      = 24,\n",
    "    num_bins        = 16,                        # Number of bins\n",
    "    threshold       = 0.2,                      # Clip the values of the descriptor\n",
    "    normalize       = 'patch'                   # Normalize at patch-level. SIFT uses cell-level descriptor normalization\n",
    ")\n",
    "\n",
    "from src.character_linking.hog import HOG\n",
    "\n",
    "\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "\n",
    "hog = HOG(hog_params)\n",
    "hogOutput = hog(first_batch.unsqueeze(1).to(dtype=torch.float32, device='cuda'))\n",
    "\n",
    "histograms_first_batch = hogOutput.histograms[:, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9082c",
   "metadata": {},
   "source": [
    "fft convolution ; scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09c2c27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing patch 240\n",
      "Grid: 8 x 7 cells, each with 16 orientation bins\n",
      "Cell size: 24 x 24 pixels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 17:33:20,206 - WARNING - /home/mathis/Bureau/OCRProject/notebook_utils/descriptor.py:265: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "\n",
      "2026-01-13 17:33:20,298 - WARNING - /home/mathis/Bureau/OCRProject/notebook_utils/descriptor.py:293: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Histogram Statistics:\n",
      "  Min: 0.0000\n",
      "  Max: 0.2011\n",
      "  Mean: 0.0148\n",
      "  L2 norm: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for _ in range(10):\n",
    "i = np.random.randint(0, len(histograms_first_batch))\n",
    "    # i = 98\n",
    "    # i = 220\n",
    "\n",
    "fig = visualize_hog(hog_params, histograms_first_batch, first_batch, hogOutput, i)\n",
    "    # savefig(fig, 'hog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fb29c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.character_linking.hog import HOG\n",
    "\n",
    "\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "\n",
    "hog = HOG(hog_params)\n",
    "\n",
    "# Get the total number of samples and histogram shape\n",
    "total_samples = len(dataloader.dataset)\n",
    "sample_output = hog(first_batch[:1].unsqueeze(1).to(dtype=torch.float32, device='cuda'))\n",
    "histogram_shape = sample_output.histograms[0, 0].shape\n",
    "\n",
    "histograms = torch.zeros((total_samples, *histogram_shape), device='cuda')\n",
    "\n",
    "# Fill progressively\n",
    "start_idx = 0\n",
    "for batch in tqdm(dataloader):\n",
    "    hogOutput = hog(batch.unsqueeze(1).to(dtype=torch.float32, device='cuda'))\n",
    "    histogram_batch = hogOutput.histograms[:, 0]\n",
    "    \n",
    "    batch_size = histogram_batch.shape[0]\n",
    "    histograms[start_idx:start_idx + batch_size] = histogram_batch\n",
    "    start_idx += batch_size\n",
    "    \n",
    "patches_df['histogram'] = histograms.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e327a",
   "metadata": {},
   "source": [
    "**Let's look at a t-sne plot of the histograms to see if we can observe some clusters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89b44241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = patches_df['histogram'].reshape(histograms.shape[0], -1)[:10000]\n",
    "# X.shape\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.scatter(X_embedded[:, 0], X_embedded[:, 1], s=1)\n",
    "# plt.xlabel('t-SNE 1')\n",
    "# plt.ylabel('t-SNE 2')\n",
    "# plt.title('t-SNE Plot')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987c26e",
   "metadata": {},
   "source": [
    "**We apply d-reduction to run our algorithms faster:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13b12a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = rearrange(histograms.cpu(), 'B N_h N_b -> B (N_h N_b)')\n",
    "\n",
    "# N_KEPT = 100\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=300)\n",
    "# pca.fit(X)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# cum_var = pca.explained_variance_ratio_.cumsum()\n",
    "# kept_var = cum_var[N_KEPT - 1]\n",
    "\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.stairs(cum_var, linewidth=2)\n",
    "# plt.axhline(0.9, color='r', linestyle='--', alpha=0.5, label='90% variance')\n",
    "# plt.axvline(N_KEPT, color='g', linestyle='--', alpha=0.5, \n",
    "#             label=f'{N_KEPT} components ({kept_var:.1%} var)')\n",
    "# plt.axhline(kept_var, color='g', linestyle=':', alpha=0.3)\n",
    "# plt.xlabel('N components')\n",
    "# plt.ylabel('Cumulative explained variance')\n",
    "# plt.title('PCA Explained Variance')\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Project onto N_KEPT components and reconstruct to original space\n",
    "# X_reduced = pca.transform(X)[:, :N_KEPT]\n",
    "# X_reconstructed = X_reduced @ pca.components_[:N_KEPT] + pca.mean_\n",
    "\n",
    "# histograms = rearrange(torch.tensor(X_reconstructed), 'B (N_h N_b) -> B N_h N_b', \n",
    "#                        N_h=histograms.shape[1], N_b=histograms.shape[2]).to(device=histograms.device,dtype=histograms.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dff14af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = histograms.reshape(histograms.shape[0], -1).cpu().numpy()\n",
    "# X.shape\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# plt.scatter(X_embedded[:, 0], X_embedded[:, 1], s=1)\n",
    "# plt.xlabel('t-SNE 1')\n",
    "# plt.ylabel('t-SNE 2')\n",
    "# plt.title('t-SNE Plot')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b84b551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = np.random.randint(0, len(histograms_first_batch))\n",
    "# hog = HOG(hog_params)\n",
    "# hogOutput = hog(first_batch.unsqueeze(1).to(dtype=torch.float32, device='cuda'))\n",
    "\n",
    "# # visualize_hog(hog_params, histograms_first_batch, first_batch, hogOutput, i)\n",
    "\n",
    "# X = rearrange(histograms_first_batch.cpu(), 'B N_h N_b -> B (N_h N_b)')\n",
    "# X_reduced = pca.transform(X)[:, :N_KEPT]\n",
    "# X_reconstructed = X_reduced @ pca.components_[:N_KEPT] + pca.mean_\n",
    "\n",
    "# histograms_proj = rearrange(torch.tensor(X_reconstructed), 'B (N_h N_b) -> B N_h N_b', \n",
    "#                        N_h=histograms.shape[1], N_b=histograms.shape[2]).to(device=histograms.device,dtype=histograms.dtype)\n",
    "\n",
    "\n",
    "# viz_only_hog(hog_params, histograms_first_batch, histograms_proj, img=first_batch[i], patch_idx=i)\n",
    "# print('a')\n",
    "# # visualize_hog(hog_params, histograms_proj, first_batch, hogOutput, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03b13991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving column: bin_patch\n",
      "Saving column: img_patch\n",
      "Saving column: page\n",
      "Saving column: file\n",
      "Saving column: left\n",
      "Saving column: top\n",
      "Saving column: width\n",
      "Saving column: height\n",
      "Saving column: label\n",
      "Saving column: filtered_bin_patch\n",
      "Saving column: svg\n",
      "Saving column: ensemble_pred\n",
      "Saving column: ensemble_uncertainty\n",
      "Saving column: ensemble_probs\n",
      "Saving column: individual_preds\n",
      "Saving column: predicted_char\n",
      "Saving column: histogram\n",
      "✓ Saved to data/processed/book1_columnwise\n"
     ]
    }
   ],
   "source": [
    "patches_df['svg'] = svg_imgs\n",
    "patches_df['histogram'] = list(histograms.cpu().numpy())\n",
    "from notebook_utils.parquet_utils import save_dataframe\n",
    "save_dataframe(patches_df, 'data/processed/book1_columnwise')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projetOCR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
