{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82cef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6075/1901343020.py:5: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../confs\", job_name=\"notebook\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from hydra import initialize, compose\n",
    "initialize(config_path=\"../confs\", job_name=\"notebook\")\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "# We change the working directory to the root of the project\n",
    "# Run this only once\n",
    "root_path = Path.cwd().parent\n",
    "os.chdir(root_path)\n",
    "sys.path.append(root_path / \"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff857b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import torch_to_pil\n",
    "from src.character_linking.feature_matching import featureMatching\n",
    "from src.character_linking.params import HOGParameters, featureMatchingParameters, fullHOGOutput, featureMatchingOutputs\n",
    "from src.utils import connectedComponent\n",
    "from src.patch_processing.patch_extraction import extract_patches\n",
    "\n",
    "from notebook_utils.viz import show_random_sample, savefig\n",
    "from notebook_utils.descriptor import compute_hog, visualize_hog\n",
    "from einops import rearrange\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "image_folder = Path('data/datasets/book_small')\n",
    "# comps_folder = Path('data/extracted/book1-complete/components/')\n",
    "comps_folder = Path('outputs/book_small/components/')\n",
    "\n",
    "\n",
    "assert image_folder.exists()\n",
    "assert comps_folder.exists()\n",
    "files = next(os.walk(image_folder))[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29ec0a",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "This notebook presents the pipeline used to preprocess the images and compute character-wise features. We:\n",
    "\n",
    "1. Preprocess the images using the Binary Vectorization Method by He. et al\n",
    "2. Render them into a grid, aligning the barycenters with the middle of the canvas\n",
    "3. Compute their Histogram of Oriented Gradients (HOG)\n",
    "4. Predict the character using EasyOCR's Traditional Chinese OCR\n",
    "5. Describe the meaning of the character using ChatGPT\n",
    "\n",
    "---\n",
    "\n",
    "# Part one: Pre-processing\n",
    "\n",
    "We begin by extracting image patches of individual characters from the source book and applying a series of pre-processing steps. First, the patches are vectorized using **Binary Shape Vectorization by Affine Scale-Space**, a method developed by Yuchen He.\n",
    "\n",
    "This technique converts binary input images into smooth SVG representations composed of Bézier curves. As a result, we obtain cleaner and smoother hànzì shapes, free from noise and JPEG compression artifacts.\n",
    "\n",
    "Subsequently, we render them at a higher resolution and apply additional filtering steps to remove ink defects and spurious artifacts within the character shapes.\n",
    "\n",
    "We now proceed by extracting the character patches and their corresponding binarized images from the document. Note that the binarized images are directly taken from the outputs of the hànzì extraction notebook. Regions predicted as not belonging to the actual character are excluded from the displayed patches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a722395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "patches_df = pd.DataFrame(columns=['bin_patch', 'img_patch', 'page', 'file', 'left', 'top', 'width', 'height', 'label'])\n",
    "\n",
    "for i, file in tqdm.tqdm(list(enumerate(files))):\n",
    "    # Load the image / components\n",
    "    img_np = np.array(Image.open(image_folder / file))\n",
    "    img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)[..., None]\n",
    "    # img_torch = torch.tensor(img_np, device=patches_device, dtype=patches_dtype).permute(2,0,1).float() / 255\n",
    "    # img_torch.requires_grad = False\n",
    "    img_comp = connectedComponent.load(comps_folder / (str(file) + '.npz'))\n",
    "    img_comp._stats = img_comp._compute_stats_from_labels(img_comp._labels)\n",
    "    \n",
    "    _bin_patches, _img_patches = extract_patches(\n",
    "        characterComponents=img_comp,\n",
    "        images = [img_np],\n",
    "        return_bin=True\n",
    "    )\n",
    "\n",
    "    lbls = [region.label for region in img_comp.regions]\n",
    "    lbls = list(filter(lambda x: not img_comp.is_deleted(x), lbls))\n",
    "\n",
    "\n",
    "    stats = img_comp.stats[1:]\n",
    "\n",
    "    page_df = pd.DataFrame({\n",
    "        'bin_patch': _bin_patches,\n",
    "        'img_patch': _img_patches,\n",
    "        'page': i,\n",
    "        'file': file,\n",
    "        'left': stats[:,0],\n",
    "        'top': stats[:, 1],\n",
    "        'width': stats[:,2],\n",
    "        'height': stats[:, 3],\n",
    "        'label': lbls\n",
    "    })\n",
    "    \n",
    "    # Concatenate immediately\n",
    "    patches_df = pd.concat([patches_df, page_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad60bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1486/1486 [00:01<00:00, 746.21it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_images = []\n",
    "\n",
    "from src.patch_processing import filter_binary_patch\n",
    "\n",
    "min_size_black=75,\n",
    "min_size_white=20,\n",
    "\n",
    "for bin_patch in tqdm.tqdm(patches_df['bin_patch']):\n",
    "    #? Filter\n",
    "    filtered = filter_binary_patch(\n",
    "        bin_patch, \n",
    "        min_size_black=min_size_black, \n",
    "        min_size_white=min_size_white, \n",
    "    )\n",
    "\n",
    "    filtered_images.append(filtered)\n",
    "\n",
    "patches_df['filtered_bin_patch'] = filtered_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdadc8b",
   "metadata": {},
   "source": [
    "**We now proceed to vectorizing the hànzi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d17275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 17:21:09,613 - INFO - Initialized vectorizer:\n",
      "2026-01-13 17:21:09,613 - INFO -   output_size: (512, 512)\n",
      "2026-01-13 17:21:09,614 - INFO -   background_color: (255, 255, 255, 255)\n",
      "2026-01-13 17:21:09,614 - INFO -   output_format: L\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "_target_: src.vectorization.BinaryShapeVectorizer\n",
       "config:\n",
       "  _target_: src.vectorization.VectorizerConfig\n",
       "  executable_path: ./build/main\n",
       "  smoothing_scale: 0.6\n",
       "  accuracy_threshold: 1\n",
       "  refinement_iterations: 0\n",
       "  output_type: shape_merged\n",
       "  return_svg: true\n",
       "  return_svg_string: false\n",
       "  return_rendered: false\n",
       "  save_dir: ./outputs_svg\n",
       "  output_size:\n",
       "  - 512\n",
       "  - 512\n",
       "  background_color:\n",
       "  - 255\n",
       "  - 255\n",
       "  - 255\n",
       "  - 255\n",
       "  dpi: 2048\n",
       "  scale: 1.0\n",
       "  output_format: L\n",
       "  n_jobs: -1\n",
       "  show_progress: true\n",
       "  chunk_size: 100\n",
       "  stream_results: true\n",
       "  stream_parallel: true\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 17:21:09,653 - INFO - Processing 1486 binarized images\n",
      "2026-01-13 17:21:09,654 - INFO - First image shape: (115, 129), dtype: bool\n",
      "Vectorizing images: 100%|\u001b[32m██████████\u001b[0m| 1486/1486 [00:26<00:00, 56.57img/s]\n",
      "Computing bounding boxes: 100%|\u001b[34m██████████\u001b[0m| 1486/1486 [00:00<00:00, 43316.88svg/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image with max aspect ratio:  1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from hydra.utils import instantiate\n",
    "from operator import itemgetter\n",
    "from omegaconf import OmegaConf\n",
    "from src.patch_processing.normalization import compute_normalization_homography, compute_svg_normalization_homography\n",
    "from src.patch_processing import filter_binary_patch\n",
    "from IPython.display import SVG as IPYSVG\n",
    "from tqdm import tqdm\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
    "\n",
    "from src.patch_processing.svg import SVG\n",
    "\n",
    "# ==== Instantiate the vectorizer ====\n",
    "\n",
    "#? See configs/vectorizer.yaml\n",
    "cfg = compose(config_name=\"vectorizer\")\n",
    "vect = instantiate(cfg)\n",
    "\n",
    "#? We show the content of the config \n",
    "display(Markdown(f\"```yaml\\n{OmegaConf.to_yaml(cfg)}\\n```\"))\n",
    "\n",
    "\n",
    "#! ==== CACHING ====\n",
    "\n",
    "if False:\n",
    "    svg_imgs = []\n",
    "    for i in tqdm(range(len(patches_df)), desc=\"Loading vectorized images\", unit=\"img\", colour='green'):\n",
    "        svg_imgs.append(SVG.load(Path(cfg.config.save_dir) / f'output_{i:06d}.svg'))\n",
    "\n",
    "#! ==== Vectorize the images ===\n",
    "\n",
    "else:\n",
    "    inverted = [p<.5 for p in patches_df['filtered_bin_patch']]\n",
    "    svg_imgs = []\n",
    "    for svg in tqdm(vect(inverted), total=len(inverted), desc=\"Vectorizing images\", unit=\"img\", colour='green'): # <-- iterator over the images | gathers data in parallel and yields it\n",
    "        svg_imgs.append(svg)\n",
    "\n",
    "    del inverted # free some memory\n",
    "\n",
    "    svg_imgs = sorted(svg_imgs, key=itemgetter(0))\n",
    "    svg_imgs = [el[1] for el in svg_imgs]\n",
    "\n",
    "patches_df['svg'] = svg_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a92ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering: 100%|██████████| 1486/1486 [00:05<00:00, 265.81img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canvas dimensions: w=168, h=192, cx=86, cy=91\n",
      "Canvas size: 168×192 pixels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 17:33:13,131 - WARNING - /tmp/ipykernel_6075/772028763.py:208: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('report_figures/canvas/fig-000008.png')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.patch_processing.normalization import compute_moment\n",
    "from src.patch_processing import filter_binary_patch\n",
    "from torch.utils.data import Dataset\n",
    "from src.patch_processing.renderer import Renderer\n",
    "\n",
    "svg_imgs = patches_df['svg']\n",
    "\n",
    "dpi = 256\n",
    "scale = 1\n",
    "\n",
    "dataset = Renderer(\n",
    "    svg_imgs=patches_df['svg'],\n",
    "    scale=scale,\n",
    "    dpi=dpi,\n",
    "    bin_thresh=128,\n",
    "    pad_to_multiple=24\n",
    ")\n",
    "\n",
    "# Access canvas dimensions\n",
    "canvas_width = dataset.canvas_width\n",
    "canvas_height = dataset.canvas_height\n",
    "center_x = dataset.center_x\n",
    "center_y = dataset.center_y\n",
    "\n",
    "print(f\"Canvas size: {canvas_width}×{canvas_height} pixels\")\n",
    "\n",
    "# Create DataLoader for batching\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Set to >0 if rendering is CPU-intensive\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# Or get individual images\n",
    "N = 10\n",
    "fig, axes = plt.subplots(N, N, figsize=(24, 24), constrained_layout=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.axis('off')\n",
    "    \n",
    "    img = dataset[i].numpy()  # Get i-th image\n",
    "    ax.imshow(img, cmap='gray', vmin=0, vmax=1, aspect='equal')\n",
    "    ax.set_title(f'Image {i}', fontsize=10)\n",
    "    \n",
    "    ax.plot(center_x, center_y, 'r+', markersize=10, markeredgewidth=1.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "savefig(fig, 'canvas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3892761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathis/anaconda3/envs/projetOCR/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-13 17:25:43,214 - INFO - use model: /home/mathis/.cnocr/2.3/densenet_lite_136-gru/cnocr-v2.3-densenet_lite_136-gru-epoch=004-ft-model.onnx\n",
      "OCR Processing: 100%|██████████| 1486/1486 [05:11<00:00,  4.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.ocr.wrappers import available_wrappers, EnsembleOCR\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# initialize wrappers and ensemble\n",
    "wrappers = available_wrappers()              # dict[name -> model]\n",
    "ensemble = EnsembleOCR(wrappers)\n",
    "\n",
    "# prepare columns in the dataframe\n",
    "patches_df[\"ensemble_pred\"] = \"\"\n",
    "patches_df[\"ensemble_uncertainty\"] = 0.0\n",
    "patches_df[\"ensemble_probs\"] = None          # dict: char -> prob\n",
    "patches_df[\"individual_preds\"] = None        # dict: model -> {char, score, uncertainty}\n",
    "\n",
    "for idx in tqdm(range(len(patches_df)), desc=\"OCR Processing\"):\n",
    "    patch = patches_df[\"svg\"].iloc[idx].render(output_format=\"L\")\n",
    "\n",
    "    results = ensemble.predict([patch])\n",
    "\n",
    "    # ----- ensemble output -----\n",
    "    ens = results[\"ensemble\"][0]\n",
    "\n",
    "    patches_df.at[idx, \"ensemble_pred\"] = ens[\"char\"]\n",
    "    patches_df.at[idx, \"ensemble_uncertainty\"] = ens[\"uncertainty\"]\n",
    "    patches_df.at[idx, \"ensemble_probs\"] = ens[\"probs\"]\n",
    "\n",
    "    # ----- individual outputs -----\n",
    "    indiv_logged = {}\n",
    "\n",
    "    for model_name, preds in results[\"individual\"].items():\n",
    "        p = preds[0]\n",
    "        indiv_logged[model_name] = {\n",
    "            \"char\": p[\"char\"],\n",
    "            \"score\": p[\"score\"],\n",
    "            \"uncertainty\": 1.0 - p[\"score\"],\n",
    "        }\n",
    "\n",
    "    patches_df.at[idx, \"individual_preds\"] = indiv_logged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e970a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "\n",
    "# import notebook_utils.parquet_utils as pu\n",
    "# importlib.reload(pu)\n",
    "# from notebook_utils.parquet_utils import load_columns\n",
    "\n",
    "# ocr_columns = ['ensemble_pred', 'ensemble_uncertainty', 'ensemble_probs', 'individual_preds', 'predicted_char']\n",
    "# ocr_data = load_columns('data/tmp/svg', ocr_columns)\n",
    "\n",
    "# for col in ocr_columns:\n",
    "#     patches_df[col] = ocr_data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd0126e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing OCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OCR Processing: 100%|██████████| 1486/1486 [01:13<00:00, 20.31it/s]\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "reader = easyocr.Reader(['ch_tra'], gpu=True, verbose=False)\n",
    "\n",
    "print(\"Processing OCR...\")\n",
    "all_results = []\n",
    "\n",
    "for idx in tqdm(range(len(patches_df)), desc=\"OCR Processing\"):\n",
    "    patch = patches_df['svg'].iloc[idx].render(output_format='RGB')\n",
    "    if patch.dtype != np.uint8:\n",
    "        patch = patch.astype(np.uint8)\n",
    "    \n",
    "    try:\n",
    "        result = reader.readtext(patch, detail=0)\n",
    "        all_results.append(result[0] if result else '')\n",
    "    except:\n",
    "        all_results.append('')\n",
    "\n",
    "patches_df['predicted_char'] = all_results\n",
    "\n",
    "del reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2a3cb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving column: bin_patch\n",
      "Saving column: img_patch\n",
      "Saving column: page\n",
      "Saving column: file\n",
      "Saving column: left\n",
      "Saving column: top\n",
      "Saving column: width\n",
      "Saving column: height\n",
      "Saving column: label\n",
      "Saving column: filtered_bin_patch\n",
      "Saving column: ensemble_pred\n",
      "Saving column: ensemble_uncertainty\n",
      "Saving column: ensemble_probs\n",
      "Saving column: individual_preds\n",
      "Saving column: predicted_char\n",
      "✓ Saved to data/tmp/svg\n"
     ]
    }
   ],
   "source": [
    "from notebook_utils.parquet_utils import save_dataframe\n",
    "save_dataframe(patches_df.drop(columns='svg'), 'data/tmp/svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "035d97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from notebook_utils.parquet_utils import load_dataframe\n",
    "# from src.patch_processing.svg import SVG\n",
    "\n",
    "# patches_df = load_dataframe('data/tmp/svg')\n",
    "# patches_df['svg'] = patches_df['svg_str'].map(lambda s: SVG.load_from_string(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b7239",
   "metadata": {},
   "source": [
    "## Computing the HOG descriptor\n",
    "\n",
    "We now explain how we compute the HOG descriptor to match the different patches to characters.\n",
    "\n",
    "The computation is done as follows:\n",
    "\n",
    "1. Preprocessing step\n",
    "\n",
    "The images on which we wish to compute the descriptors are preprocessed.\n",
    "It is possible to compute the descriptors on either grayscale images or multi-channel images. We use grayscale conversion.\n",
    "We also resize the images to a fixed size.\n",
    "\n",
    "2. Computing the gradients\n",
    "\n",
    "There are several ways to compute the gradients. We first begin by using a gaussian filter to smooth the image.\n",
    "Then, we apply a separable convolution based on cv2.getDerivKernels to compute the image gradients. The two components of this separable convolutions are:\n",
    "- A smoothing operator on one axis\n",
    "- It's derivative on the other axis\n",
    "\n",
    "This is justified because of derivative operations on convolution products.:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x}(I * G) = I * \\frac{\\partial G}{\\partial x}$$\n",
    "\n",
    "For a 2D Gaussian $G(x,y) = G_x(x) \\cdot G_y(y)$, the x-gradient becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x}(I * G) = I * \\left(\\frac{\\partial G_x}{\\partial x} \\otimes G_y\\right)\n",
    "$$\n",
    "\n",
    "where $\\otimes$ denotes separable convolution: first convolve rows with $G_y$ (smooth), then columns with $\\frac{\\partial G_x}{\\partial x}$ (differentiate).\n",
    "\n",
    "3. Histogram Computation\n",
    "\n",
    "We begin by computing:\n",
    "- The orientation of the gradients in the image: $\\theta = \\text{arctan2}(dx, dy)$\n",
    "- Their magnitude: $M = \\sqrt{{dx}^2 + {dy}^2}$\n",
    "\n",
    "We rearrange these arrays of shape $(H, W)$ to $(\\text{Nh}, \\text{Nw}, h, w)$, with Nh and Nw the number of rows / cols of cells, $h$ and $w$ their height and width\n",
    "\n",
    "We will use trilinear interpolation to compute the histograms.\n",
    "For each pixel $u$ of the cell, we compute the two bins the cell should contribute to:\n",
    "\n",
    "$$i_1 = \\lfloor\\theta_u \\cdot N_{\\text{bins}}\\rfloor \\quad ; \\quad i_2 = i_1 + 1$$\n",
    "\n",
    "The contribution weights for angular interpolation are:\n",
    "$$w_2 = \\theta_u \\cdot N_{\\text{bins}} - i_1 \\quad ; \\quad w_1 = 1 - w_2$$\n",
    "\n",
    "Optionally, a Gaussian spatial weighting can be applied (used in SIFT, but not in standard HOG):\n",
    "$$w_{\\text{spatial}}(x, y) = \\exp\\left(-\\frac{x_n^2 + y_n^2}{2\\sigma^2}\\right)$$\n",
    "where $(x_n, y_n) \\in [-1, 1]^2$ are normalized coordinates from the cell center.\n",
    "\n",
    "Each pixel contributes to the histogram:\n",
    "$$H[i_1] \\mathrel{+}= M_u \\cdot w_1 \\cdot w_{\\text{spatial}} \\quad ; \\quad H[i_2] \\mathrel{+}= M_u \\cdot w_2 \\cdot w_{\\text{spatial}}$$\n",
    "\n",
    "4. Normalization\n",
    "\n",
    "As in SIFT, the descriptors can be optionally normalized.\n",
    "We can normalize them:\n",
    "- At cell-level: normalize each histogram to be unit norm (as done in SIFT)\n",
    "- At patch-level: normalize the vector of concatenated\n",
    "\n",
    "[This paper](https://www.ipol.im/pub/art/2014/82/article.pdf), Anatomy of the SIFT method, states that cell-level normalization can help with lightning changes.\n",
    "We don't have such lightning changes, and have small cells. We therefore prefeer to use patch-level normalization. \n",
    "\n",
    "As in SIFT, we clip the values of the descriptors to be in the $[-0.2, 0.2]$ range, before re-normalizing the descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dfbecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_params = HOGParameters(\n",
    "    device          = \"cuda\",\n",
    "    C               = 1,                        # Use grayscale images\n",
    "    partial_output  = False,                    # Also output the resized images, their gradient orientation and magnitude\n",
    "    method          = 'gaussian',               # Use gaussian smoothing to compute the gradients\n",
    "    grdt_sigma      = 5,                      # Std of the smoothing\n",
    "    ksize_factor    = 6,                        # Size of the smoothing kernel = factor * std\n",
    "    cell_height     = 24,                       # Size of the cells to compute the histograms\n",
    "    cell_width      = 24,\n",
    "    num_bins        = 16,                        # Number of bins\n",
    "    threshold       = 0.2,                      # Clip the values of the descriptor\n",
    "    normalize       = 'patch'                   # Normalize at patch-level. SIFT uses cell-level descriptor normalization\n",
    ")\n",
    "\n",
    "from src.character_linking.hog import HOG\n",
    "\n",
    "\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "\n",
    "hog = HOG(hog_params)\n",
    "hogOutput = hog(first_batch.unsqueeze(1).to(dtype=torch.float32, device='cuda'))\n",
    "\n",
    "histograms_first_batch = hogOutput.histograms[:, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9082c",
   "metadata": {},
   "source": [
    "fft convolution ; scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09c2c27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing patch 240\n",
      "Grid: 8 x 7 cells, each with 16 orientation bins\n",
      "Cell size: 24 x 24 pixels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 17:33:20,206 - WARNING - /home/mathis/Bureau/OCRProject/notebook_utils/descriptor.py:265: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "\n",
      "2026-01-13 17:33:20,298 - WARNING - /home/mathis/Bureau/OCRProject/notebook_utils/descriptor.py:293: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Histogram Statistics:\n",
      "  Min: 0.0000\n",
      "  Max: 0.2011\n",
      "  Mean: 0.0148\n",
      "  L2 norm: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for _ in range(10):\n",
    "i = np.random.randint(0, len(histograms_first_batch))\n",
    "    # i = 98\n",
    "    # i = 220\n",
    "\n",
    "fig = visualize_hog(hog_params, histograms_first_batch, first_batch, hogOutput, i)\n",
    "    # savefig(fig, 'hog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fb29c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.character_linking.hog import HOG\n",
    "\n",
    "\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "\n",
    "hog = HOG(hog_params)\n",
    "\n",
    "# Get the total number of samples and histogram shape\n",
    "total_samples = len(dataloader.dataset)\n",
    "sample_output = hog(first_batch[:1].unsqueeze(1).to(dtype=torch.float32, device='cuda'))\n",
    "histogram_shape = sample_output.histograms[0, 0].shape\n",
    "\n",
    "histograms = torch.zeros((total_samples, *histogram_shape), device='cuda')\n",
    "\n",
    "# Fill progressively\n",
    "start_idx = 0\n",
    "for batch in tqdm(dataloader):\n",
    "    hogOutput = hog(batch.unsqueeze(1).to(dtype=torch.float32, device='cuda'))\n",
    "    histogram_batch = hogOutput.histograms[:, 0]\n",
    "    \n",
    "    batch_size = histogram_batch.shape[0]\n",
    "    histograms[start_idx:start_idx + batch_size] = histogram_batch\n",
    "    start_idx += batch_size\n",
    "    \n",
    "patches_df['histogram'] = histograms.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e327a",
   "metadata": {},
   "source": [
    "**Let's look at a t-sne plot of the histograms to see if we can observe some clusters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89b44241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = patches_df['histogram'].reshape(histograms.shape[0], -1)[:10000]\n",
    "# X.shape\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.scatter(X_embedded[:, 0], X_embedded[:, 1], s=1)\n",
    "# plt.xlabel('t-SNE 1')\n",
    "# plt.ylabel('t-SNE 2')\n",
    "# plt.title('t-SNE Plot')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987c26e",
   "metadata": {},
   "source": [
    "**We apply d-reduction to run our algorithms faster:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13b12a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = rearrange(histograms.cpu(), 'B N_h N_b -> B (N_h N_b)')\n",
    "\n",
    "# N_KEPT = 100\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=300)\n",
    "# pca.fit(X)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# cum_var = pca.explained_variance_ratio_.cumsum()\n",
    "# kept_var = cum_var[N_KEPT - 1]\n",
    "\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.stairs(cum_var, linewidth=2)\n",
    "# plt.axhline(0.9, color='r', linestyle='--', alpha=0.5, label='90% variance')\n",
    "# plt.axvline(N_KEPT, color='g', linestyle='--', alpha=0.5, \n",
    "#             label=f'{N_KEPT} components ({kept_var:.1%} var)')\n",
    "# plt.axhline(kept_var, color='g', linestyle=':', alpha=0.3)\n",
    "# plt.xlabel('N components')\n",
    "# plt.ylabel('Cumulative explained variance')\n",
    "# plt.title('PCA Explained Variance')\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Project onto N_KEPT components and reconstruct to original space\n",
    "# X_reduced = pca.transform(X)[:, :N_KEPT]\n",
    "# X_reconstructed = X_reduced @ pca.components_[:N_KEPT] + pca.mean_\n",
    "\n",
    "# histograms = rearrange(torch.tensor(X_reconstructed), 'B (N_h N_b) -> B N_h N_b', \n",
    "#                        N_h=histograms.shape[1], N_b=histograms.shape[2]).to(device=histograms.device,dtype=histograms.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dff14af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = histograms.reshape(histograms.shape[0], -1).cpu().numpy()\n",
    "# X.shape\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# plt.scatter(X_embedded[:, 0], X_embedded[:, 1], s=1)\n",
    "# plt.xlabel('t-SNE 1')\n",
    "# plt.ylabel('t-SNE 2')\n",
    "# plt.title('t-SNE Plot')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b84b551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = np.random.randint(0, len(histograms_first_batch))\n",
    "# hog = HOG(hog_params)\n",
    "# hogOutput = hog(first_batch.unsqueeze(1).to(dtype=torch.float32, device='cuda'))\n",
    "\n",
    "# # visualize_hog(hog_params, histograms_first_batch, first_batch, hogOutput, i)\n",
    "\n",
    "# X = rearrange(histograms_first_batch.cpu(), 'B N_h N_b -> B (N_h N_b)')\n",
    "# X_reduced = pca.transform(X)[:, :N_KEPT]\n",
    "# X_reconstructed = X_reduced @ pca.components_[:N_KEPT] + pca.mean_\n",
    "\n",
    "# histograms_proj = rearrange(torch.tensor(X_reconstructed), 'B (N_h N_b) -> B N_h N_b', \n",
    "#                        N_h=histograms.shape[1], N_b=histograms.shape[2]).to(device=histograms.device,dtype=histograms.dtype)\n",
    "\n",
    "\n",
    "# viz_only_hog(hog_params, histograms_first_batch, histograms_proj, img=first_batch[i], patch_idx=i)\n",
    "# print('a')\n",
    "# # visualize_hog(hog_params, histograms_proj, first_batch, hogOutput, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03b13991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving column: bin_patch\n",
      "Saving column: img_patch\n",
      "Saving column: page\n",
      "Saving column: file\n",
      "Saving column: left\n",
      "Saving column: top\n",
      "Saving column: width\n",
      "Saving column: height\n",
      "Saving column: label\n",
      "Saving column: filtered_bin_patch\n",
      "Saving column: svg\n",
      "Saving column: ensemble_pred\n",
      "Saving column: ensemble_uncertainty\n",
      "Saving column: ensemble_probs\n",
      "Saving column: individual_preds\n",
      "Saving column: predicted_char\n",
      "Saving column: histogram\n",
      "✓ Saved to data/processed/book1_columnwise\n"
     ]
    }
   ],
   "source": [
    "patches_df['svg'] = svg_imgs\n",
    "patches_df['histogram'] = list(histograms.cpu().numpy())\n",
    "from notebook_utils.parquet_utils import save_dataframe\n",
    "save_dataframe(patches_df, 'data/processed/book1_columnwise')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projetOCR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
